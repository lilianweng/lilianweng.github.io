<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Flow-based Deep Generative Models | Lil&#39;Log</title>
<meta name="keywords" content="architecture, generative-model, image-generation, math-heavy" />
<meta name="description" content="
So far, I&rsquo;ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$.">
<meta name="author" content="Lilian Weng">
<link rel="canonical" href="https://lilianweng.github.io/posts/2018-10-13-flow-models/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.51b2420ff5ea1215cdf584af7ba59d5fea94201c33f25109d6448c7271631316.css" integrity="sha256-UbJCD/XqEhXN9YSve6WdX&#43;qUIBwz8lEJ1kSMcnFjExY=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js" integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lilianweng.github.io/favicon_wine.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lilianweng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lilianweng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lilianweng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lilianweng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://lilianweng.github.io/posts/2018-10-13-flow-models/" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-HFT45VFBX6');
        }
      </script><meta property="og:title" content="Flow-based Deep Generative Models" />
<meta property="og:description" content="
So far, I&rsquo;ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lilianweng.github.io/posts/2018-10-13-flow-models/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-13T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2018-10-13T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Flow-based Deep Generative Models"/>
<meta name="twitter:description" content="
So far, I&rsquo;ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lilianweng.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Flow-based Deep Generative Models",
      "item": "https://lilianweng.github.io/posts/2018-10-13-flow-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Flow-based Deep Generative Models",
  "name": "Flow-based Deep Generative Models",
  "description": " So far, I\u0026rsquo;ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\\mathbf{x})$ (where $\\mathbf{x} \\in \\mathcal{D}$) \u0026mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\\mathbf{x}) = \\int p(\\mathbf{x}\\vert\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\\mathbf{z}$.\n",
  "keywords": [
    "architecture", "generative-model", "image-generation", "math-heavy"
  ],
  "articleBody": " So far, I’ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\\mathbf{x})$ (where $\\mathbf{x} \\in \\mathcal{D}$) — because it is really hard! Taking the generative model with latent variables as an example, $p(\\mathbf{x}) = \\int p(\\mathbf{x}\\vert\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\\mathbf{z}$.\nFlow-based deep generative models conquer this hard problem with the help of normalizing flows, a powerful statistics tool for density estimation. A good estimation of $p(\\mathbf{x})$ makes it possible to efficiently complete many downstream tasks: sample unobserved but realistic new data points (data generation), predict the rareness of future events (density estimation), infer latent variables, fill in incomplete data samples, etc.\nTypes of Generative Models Here is a quick summary of the difference between GAN, VAE, and flow-based generative models:\nGenerative adversarial networks: GAN provides a smart solution to model the data generation, an unsupervised learning problem, as a supervised one. The discriminator model learns to distinguish the real data from the fake samples that are produced by the generator model. Two models are trained as they are playing a minimax game. Variational autoencoders: VAE inexplicitly optimizes the log-likelihood of the data by maximizing the evidence lower bound (ELBO). Flow-based generative models: A flow-based generative model is constructed by a sequence of invertible transformations. Unlike other two, the model explicitly learns the data distribution $p(\\mathbf{x})$ and therefore the loss function is simply the negative log-likelihood. Comparison of three categories of generative models. Linear Algebra Basics Recap We should understand two key concepts before getting into the flow-based generative model: the Jacobian determinant and the change of variable rule. Pretty basic, so feel free to skip.\nJacobian Matrix and Determinant Given a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $\\mathbf{f}: \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$ where one entry on the i-th row and j-th column is $\\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j}$.\n$$ \\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026 \\dots \u0026 \\frac{\\partial f_1}{\\partial x_n} \\\\[6pt] \\vdots \u0026 \\ddots \u0026 \\vdots \\\\[6pt] \\frac{\\partial f_m}{\\partial x_1} \u0026 \\dots \u0026 \\frac{\\partial f_m}{\\partial x_n} \\\\[6pt] \\end{bmatrix} $$ The determinant is one real number computed as a function of all the elements in a squared matrix. Note that the determinant only exists for square matrices. The absolute value of the determinant can be thought of as a measure of “how much multiplication by the matrix expands or contracts space”.\nThe determinant of a nxn matrix $M$ is:\n$$ \\det M = \\det \\begin{bmatrix} a_{11} \u0026 a_{12} \u0026 \\dots \u0026 a_{1n} \\\\ a_{21} \u0026 a_{22} \u0026 \\dots \u0026 a_{2n} \\\\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\\\ a_{n1} \u0026 a_{n2} \u0026 \\dots \u0026 a_{nn} \\\\ \\end{bmatrix} = \\sum_{j_1 j_2 \\dots j_n} (-1)^{\\tau(j_1 j_2 \\dots j_n)} a_{1j_1} a_{2j_2} \\dots a_{nj_n} $$ where the subscript under the summation $j_1 j_2 \\dots j_n$ are all permutations of the set {1, 2, …, n}, so there are $n!$ items in total; $\\tau(.)$ indicates the signature of a permutation.\nThe determinant of a square matrix $M$ detects whether it is invertible: If $\\det(M)=0$ then $M$ is not invertible (a singular matrix with linearly dependent rows or columns; or any row or column is all 0); otherwise, if $\\det(M)\\neq 0$, $M$ is invertible.\nThe determinant of the product is equivalent to the product of the determinants: $\\det(AB) = \\det(A)\\det(B)$. (proof)\nChange of Variable Theorem Let’s review the change of variable theorem specifically in the context of probability density estimation, starting with a single variable case.\nGiven a random variable $z$ and its known probability density function $z \\sim \\pi(z)$, we would like to construct a new random variable using a 1-1 mapping function $x = f(z)$. The function $f$ is invertible, so $z=f^{-1}(x)$. Now the question is how to infer the unknown probability density function of the new variable, $p(x)$?\n$$ \\begin{aligned} \u0026 \\int p(x)dx = \\int \\pi(z)dz = 1 \\scriptstyle{\\text{ ; Definition of probability distribution.}}\\\\ \u0026 p(x) = \\pi(z) \\left\\vert\\frac{dz}{dx}\\right\\vert = \\pi(f^{-1}(x)) \\left\\vert\\frac{d f^{-1}}{dx}\\right\\vert = \\pi(f^{-1}(x)) \\vert (f^{-1})'(x) \\vert \\end{aligned} $$ By definition, the integral $\\int \\pi(z)dz$ is the sum of an infinite number of rectangles of infinitesimal width $\\Delta z$. The height of such a rectangle at position $z$ is the value of the density function $\\pi(z)$. When we substitute the variable, $z = f^{-1}(x)$ yields $\\frac{\\Delta z}{\\Delta x} = (f^{-1}(x))’$ and $\\Delta z = (f^{-1}(x))’ \\Delta x$. Here $\\vert(f^{-1}(x))’\\vert$ indicates the ratio between the area of rectangles defined in two different coordinate of variables $z$ and $x$ respectively.\nThe multivariable version has a similar format:\n$$ \\begin{aligned} \\mathbf{z} \u0026\\sim \\pi(\\mathbf{z}), \\mathbf{x} = f(\\mathbf{z}), \\mathbf{z} = f^{-1}(\\mathbf{x}) \\\\ p(\\mathbf{x}) \u0026= \\pi(\\mathbf{z}) \\left\\vert \\det \\dfrac{d \\mathbf{z}}{d \\mathbf{x}} \\right\\vert = \\pi(f^{-1}(\\mathbf{x})) \\left\\vert \\det \\dfrac{d f^{-1}}{d \\mathbf{x}} \\right\\vert \\end{aligned} $$ where $\\det \\frac{\\partial f}{\\partial\\mathbf{z}}$ is the Jacobian determinant of the function $f$. The full proof of the multivariate version is out of the scope of this post; ask Google if interested ;)\nWhat is Normalizing Flows? Being able to do good density estimation has direct applications in many machine learning problems, but it is very hard. For example, since we need to run backward propagation in deep learning models, the embedded probability distribution (i.e. posterior $p(\\mathbf{z}\\vert\\mathbf{x})$) is expected to be simple enough to calculate the derivative easily and efficiently. That is why Gaussian distribution is often used in latent variable generative models, even though most of real world distributions are much more complicated than Gaussian.\nHere comes a Normalizing Flow (NF) model for better and more powerful distribution approximation. A normalizing flow transforms a simple distribution into a complex one by applying a sequence of invertible transformation functions. Flowing through a chain of transformations, we repeatedly substitute the variable for the new one according to the change of variables theorem and eventually obtain a probability distribution of the final target variable.\nIllustration of a normalizing flow model, transforming a simple distribution $p\\_0(\\mathbf{z}\\_0)$ to a complex one $p\\_K(\\mathbf{z}\\_K)$ step by step. As defined in Fig. 2,\n$$ \\begin{aligned} \\mathbf{z}_{i-1} \u0026\\sim p_{i-1}(\\mathbf{z}_{i-1}) \\\\ \\mathbf{z}_i \u0026= f_i(\\mathbf{z}_{i-1})\\text{, thus }\\mathbf{z}_{i-1} = f_i^{-1}(\\mathbf{z}_i) \\\\ p_i(\\mathbf{z}_i) \u0026= p_{i-1}(f_i^{-1}(\\mathbf{z}_i)) \\left\\vert \\det\\dfrac{d f_i^{-1}}{d \\mathbf{z}_i} \\right\\vert \\end{aligned} $$ Then let’s convert the equation to be a function of $\\mathbf{z}_i$ so that we can do inference with the base distribution.\n$$ \\begin{aligned} p_i(\\mathbf{z}_i) \u0026= p_{i-1}(f_i^{-1}(\\mathbf{z}_i)) \\left\\vert \\det\\dfrac{d f_i^{-1}}{d \\mathbf{z}_i} \\right\\vert \\\\ \u0026= p_{i-1}(\\mathbf{z}_{i-1}) \\left\\vert \\det \\color{red}{\\Big(\\dfrac{d f_i}{d\\mathbf{z}_{i-1}}\\Big)^{-1}} \\right\\vert \u0026 \\scriptstyle{\\text{; According to the inverse func theorem.}} \\\\ \u0026= p_{i-1}(\\mathbf{z}_{i-1}) \\color{red}{\\left\\vert \\det \\dfrac{d f_i}{d\\mathbf{z}_{i-1}} \\right\\vert^{-1}} \u0026 \\scriptstyle{\\text{; According to a property of Jacobians of invertible func.}} \\\\ \\log p_i(\\mathbf{z}_i) \u0026= \\log p_{i-1}(\\mathbf{z}_{i-1}) - \\log \\left\\vert \\det \\dfrac{d f_i}{d\\mathbf{z}_{i-1}} \\right\\vert \\end{aligned} $$ (*) A note on the “inverse function theorem”: If $y=f(x)$ and $x=f^{-1}(y)$, we have:\n$$ \\dfrac{df^{-1}(y)}{dy} = \\dfrac{dx}{dy} = (\\dfrac{dy}{dx})^{-1} = (\\dfrac{df(x)}{dx})^{-1} $$ (*) A note on “Jacobians of invertible function”: The determinant of the inverse of an invertible matrix is the inverse of the determinant: $\\det(M^{-1}) = (\\det(M))^{-1}$, because $\\det(M)\\det(M^{-1}) = \\det(M \\cdot M^{-1}) = \\det(I) = 1$.\nGiven such a chain of probability density functions, we know the relationship between each pair of consecutive variables. We can expand the equation of the output $\\mathbf{x}$ step by step until tracing back to the initial distribution $\\mathbf{z}_0$.\n$$ \\begin{aligned} \\mathbf{x} = \\mathbf{z}_K \u0026= f_K \\circ f_{K-1} \\circ \\dots \\circ f_1 (\\mathbf{z}_0) \\\\ \\log p(\\mathbf{x}) = \\log \\pi_K(\\mathbf{z}_K) \u0026= \\log \\pi_{K-1}(\\mathbf{z}_{K-1}) - \\log\\left\\vert\\det\\dfrac{d f_K}{d \\mathbf{z}_{K-1}}\\right\\vert \\\\ \u0026= \\log \\pi_{K-2}(\\mathbf{z}_{K-2}) - \\log\\left\\vert\\det\\dfrac{d f_{K-1}}{d\\mathbf{z}_{K-2}}\\right\\vert - \\log\\left\\vert\\det\\dfrac{d f_K}{d\\mathbf{z}_{K-1}}\\right\\vert \\\\ \u0026= \\dots \\\\ \u0026= \\log \\pi_0(\\mathbf{z}_0) - \\sum_{i=1}^K \\log\\left\\vert\\det\\dfrac{d f_i}{d\\mathbf{z}_{i-1}}\\right\\vert \\end{aligned} $$ The path traversed by the random variables $\\mathbf{z}_i = f_i(\\mathbf{z}_{i-1})$ is the flow and the full chain formed by the successive distributions $\\pi_i$ is called a normalizing flow. Required by the computation in the equation, a transformation function $f_i$ should satisfy two properties:\nIt is easily invertible. Its Jacobian determinant is easy to compute. Models with Normalizing Flows With normalizing flows in our toolbox, the exact log-likelihood of input data $\\log p(\\mathbf{x})$ becomes tractable. As a result, the training criterion of flow-based generative model is simply the negative log-likelihood (NLL) over the training dataset $\\mathcal{D}$:\n$$ \\mathcal{L}(\\mathcal{D}) = - \\frac{1}{\\vert\\mathcal{D}\\vert}\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p(\\mathbf{x}) $$ RealNVP The RealNVP (Real-valued Non-Volume Preserving; Dinh et al., 2017) model implements a normalizing flow by stacking a sequence of invertible bijective transformation functions. In each bijection $f: \\mathbf{x} \\mapsto \\mathbf{y}$, known as affine coupling layer, the input dimensions are split into two parts:\nThe first $d$ dimensions stay same; The second part, $d+1$ to $D$ dimensions, undergo an affine transformation (“scale-and-shift”) and both the scale and shift parameters are functions of the first $d$ dimensions. $$ \\begin{aligned} \\mathbf{y}_{1:d} \u0026= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} \u0026= \\mathbf{x}_{d+1:D} \\odot \\exp({s(\\mathbf{x}_{1:d})}) + t(\\mathbf{x}_{1:d}) \\end{aligned} $$ where $s(.)$ and $t(.)$ are scale and translation functions and both map $\\mathbb{R}^d \\mapsto \\mathbb{R}^{D-d}$. The $\\odot$ operation is the element-wise product.\nNow let’s check whether this transformation satisfy two basic properties for a flow transformation.\nCondition 1: “It is easily invertible.”\nYes and it is fairly straightforward.\n$$ \\begin{cases} \\mathbf{y}_{1:d} \u0026= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} \u0026= \\mathbf{x}_{d+1:D} \\odot \\exp({s(\\mathbf{x}_{1:d})}) + t(\\mathbf{x}_{1:d}) \\end{cases} \\Leftrightarrow \\begin{cases} \\mathbf{x}_{1:d} \u0026= \\mathbf{y}_{1:d} \\\\ \\mathbf{x}_{d+1:D} \u0026= (\\mathbf{y}_{d+1:D} - t(\\mathbf{y}_{1:d})) \\odot \\exp(-s(\\mathbf{y}_{1:d})) \\end{cases} $$ Condition 2: “Its Jacobian determinant is easy to compute.”\nYes. It is not hard to get the Jacobian matrix and determinant of this transformation. The Jacobian is a lower triangular matrix.\n$$ \\mathbf{J} = \\begin{bmatrix} \\mathbb{I}_d \u0026 \\mathbf{0}_{d\\times(D-d)} \\\\[5pt] \\frac{\\partial \\mathbf{y}_{d+1:D}}{\\partial \\mathbf{x}_{1:d}} \u0026 \\text{diag}(\\exp(s(\\mathbf{x}_{1:d}))) \\end{bmatrix} $$ Hence the determinant is simply the product of terms on the diagonal.\n$$ \\det(\\mathbf{J}) = \\prod_{j=1}^{D-d}\\exp(s(\\mathbf{x}_{1:d}))_j = \\exp(\\sum_{j=1}^{D-d} s(\\mathbf{x}_{1:d})_j) $$ So far, the affine coupling layer looks perfect for constructing a normalizing flow :)\nEven better, since (i) computing $f^-1$ does not require computing the inverse of $s$ or $t$ and (ii) computing the Jacobian determinant does not involve computing the Jacobian of $s$ or $t$, those functions can be arbitrarily complex; i.e. both $s$ and $t$ can be modeled by deep neural networks.\nIn one affine coupling layer, some dimensions (channels) remain unchanged. To make sure all the inputs have a chance to be altered, the model reverses the ordering in each layer so that different components are left unchanged. Following such an alternating pattern, the set of units which remain identical in one transformation layer are always modified in the next. Batch normalization is found to help training models with a very deep stack of coupling layers.\nFurthermore, RealNVP can work in a multi-scale architecture to build a more efficient model for large inputs. The multi-scale architecture applies several “sampling” operations to normal affine layers, including spatial checkerboard pattern masking, squeezing operation, and channel-wise masking. Read the paper for more details on the multi-scale architecture.\nNICE The NICE (Non-linear Independent Component Estimation; Dinh, et al. 2015) model is a predecessor of RealNVP. The transformation in NICE is the affine coupling layer without the scale term, known as additive coupling layer.\n$$ \\begin{cases} \\mathbf{y}_{1:d} \u0026= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} \u0026= \\mathbf{x}_{d+1:D} + m(\\mathbf{x}_{1:d}) \\end{cases} \\Leftrightarrow \\begin{cases} \\mathbf{x}_{1:d} \u0026= \\mathbf{y}_{1:d} \\\\ \\mathbf{x}_{d+1:D} \u0026= \\mathbf{y}_{d+1:D} - m(\\mathbf{y}_{1:d}) \\end{cases} $$ Glow The Glow (Kingma and Dhariwal, 2018) model extends the previous reversible generative models, NICE and RealNVP, and simplifies the architecture by replacing the reverse permutation operation on the channel ordering with invertible 1x1 convolutions.\nOne step of flow in the Glow model. (Image source: Kingma and Dhariwal, 2018) There are three substeps in one step of flow in Glow.\nSubstep 1: Activation normalization (short for “actnorm”)\nIt performs an affine transformation using a scale and bias parameter per channel, similar to batch normalization, but works for mini-batch size 1. The parameters are trainable but initialized so that the first minibatch of data have mean 0 and standard deviation 1 after actnorm.\nSubstep 2: Invertible 1x1 conv\nBetween layers of the RealNVP flow, the ordering of channels is reversed so that all the data dimensions have a chance to be altered. A 1×1 convolution with equal number of input and output channels is a generalization of any permutation of the channel ordering.\nSay, we have an invertible 1x1 convolution of an input $h \\times w \\times c$ tensor $\\mathbf{h}$ with a weight matrix $\\mathbf{W}$ of size $c \\times c$. The output is a $h \\times w \\times c$ tensor, labeled as $f = \\texttt{conv2d}(\\mathbf{h}; \\mathbf{W})$. In order to apply the change of variable rule, we need to compute the Jacobian determinant $\\vert \\det\\partial f / \\partial\\mathbf{h}\\vert$.\nBoth the input and output of 1x1 convolution here can be viewed as a matrix of size $h \\times w$. Each entry $\\mathbf{x}_{ij}$ ($i=1,\\dots,h, j=1,\\dots,w$) in $\\mathbf{h}$ is a vector of $c$ channels and each entry is multiplied by the weight matrix $\\mathbf{W}$ to obtain the corresponding entry $\\mathbf{y}_{ij}$ in the output matrix respectively. The derivative of each entry is $\\partial \\mathbf{x}_{ij} \\mathbf{W} / \\partial\\mathbf{x}_{ij} = \\mathbf{W}$ and there are $h \\times w$ such entries in total:\n$$ \\log \\left\\vert\\det \\frac{\\partial\\texttt{conv2d}(\\mathbf{h}; \\mathbf{W})}{\\partial\\mathbf{h}}\\right\\vert = \\log (\\vert\\det\\mathbf{W}\\vert^{h \\cdot w}\\vert) = h \\cdot w \\cdot \\log \\vert\\det\\mathbf{W}\\vert $$ The inverse 1x1 convolution depends on the inverse matrix $\\mathbf{W}^{-1}$. Since the weight matrix is relatively small, the amount of computation for the matrix determinant (tf.linalg.det) and inversion (tf.linalg.inv) is still under control.\nSubstep 3: Affine coupling layer\nThe design is same as in RealNVP.\nThree substeps in one step of flow in Glow. (Image source: Kingma and Dhariwal, 2018) Models with Autoregressive Flows The autoregressive constraint is a way to model sequential data, $\\mathbf{x} = [x_1, \\dots, x_D]$: each output only depends on the data observed in the past, but not on the future ones. In other words, the probability of observing $x_i$ is conditioned on $x_1, \\dots, x_{i-1}$ and the product of these conditional probabilities gives us the probability of observing the full sequence:\n$$ p(\\mathbf{x}) = \\prod_{i=1}^{D} p(x_i\\vert x_1, \\dots, x_{i-1}) = \\prod_{i=1}^{D} p(x_i\\vert x_{1:i-1}) $$ How to model the conditional density is of your choice. It can be a univariate Gaussian with mean and standard deviation computed as a function of $x_{1:i-1}$, or a multilayer neural network with $x_{1:i-1}$ as the input.\nIf a flow transformation in a normalizing flow is framed as an autoregressive model — each dimension in a vector variable is conditioned on the previous dimensions — this is an autoregressive flow.\nThis section starts with several classic autoregressive models (MADE, PixelRNN, WaveNet) and then we dive into autoregressive flow models (MAF and IAF).\nMADE MADE (Masked Autoencoder for Distribution Estimation; Germain et al., 2015) is a specially designed architecture to enforce the autoregressive property in the autoencoder efficiently. When using an autoencoder to predict the conditional probabilities, rather than feeding the autoencoder with input of different observation windows $D$ times, MADE removes the contribution from certain hidden units by multiplying binary mask matrices so that each input dimension is reconstructed only from previous dimensions in a given ordering in a single pass.\nIn a multilayer fully-connected neural network, say, we have $L$ hidden layers with weight matrices $\\mathbf{W}^1, \\dots, \\mathbf{W}^L$ and an output layer with weight matrix $\\mathbf{V}$. The output $\\hat{\\mathbf{x}}$ has each dimension $\\hat{x}_i = p(x_i\\vert x_{1:i-1})$.\nWithout any mask, the computation through layers looks like the following:\n$$ \\begin{aligned} \\mathbf{h}^0 \u0026= \\mathbf{x} \\\\ \\mathbf{h}^l \u0026= \\text{activation}^l(\\mathbf{W}^l\\mathbf{h}^{l-1} + \\mathbf{b}^l) \\\\ \\hat{\\mathbf{x}} \u0026= \\sigma(\\mathbf{V}\\mathbf{h}^L + \\mathbf{c}) \\end{aligned} $$ Demonstration of how MADE works in a three-layer feed-forward neural network. (Image source: Germain et al., 2015) To zero out some connections between layers, we can simply element-wise multiply every weight matrix by a binary mask matrix. Each hidden node is assigned with a random “connectivity integer” between $1$ and $D-1$; the assigned value for the $k$-th unit in the $l$-th layer is denoted by $m^l_k$. The binary mask matrix is determined by element-wise comparing values of two nodes in two layers.\n$$ \\begin{aligned} \\mathbf{h}^l \u0026= \\text{activation}^l((\\mathbf{W}^l \\color{red}{\\odot \\mathbf{M}^{\\mathbf{W}^l}}) \\mathbf{h}^{l-1} + \\mathbf{b}^l) \\\\ \\hat{\\mathbf{x}} \u0026= \\sigma((\\mathbf{V} \\color{red}{\\odot \\mathbf{M}^{\\mathbf{V}}}) \\mathbf{h}^L + \\mathbf{c}) \\\\ M^{\\mathbf{W}^l}_{k', k} \u0026= \\mathbf{1}_{m^l_{k'} \\geq m^{l-1}_k} = \\begin{cases} 1, \u0026 \\text{if } m^l_{k'} \\geq m^{l-1}_k\\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\\\ M^{\\mathbf{V}}_{d, k} \u0026= \\mathbf{1}_{d \\geq m^L_k} = \\begin{cases} 1, \u0026 \\text{if } d \u003e m^L_k\\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$ A unit in the current layer can only be connected to other units with equal or smaller numbers in the previous layer and this type of dependency easily propagates through the network up to the output layer. Once the numbers are assigned to all the units and layers, the ordering of input dimensions is fixed and the conditional probability is produced with respect to it. See a great illustration in To make sure all the hidden units are connected to the input and output layers through some paths, the $m^l_k$ is sampled to be equal or greater than the minimal connectivity integer in the previous layer, $\\min_{k’} m_{k’}^{l-1}$.\nMADE training can be further facilitated by:\nOrder-agnostic training: shuffle the input dimensions, so that MADE is able to model any arbitrary ordering; can create an ensemble of autoregressive models at the runtime. Connectivity-agnostic training: to avoid a model being tied up to a specific connectivity pattern constraints, resample $m^l_k$ for each training minibatch. PixelRNN PixelRNN (Oord et al, 2016) is a deep generative model for images. The image is generated one pixel at a time and each new pixel is sampled conditional on the pixels that have been seen before.\nLet’s consider an image of size $n \\times n$, $\\mathbf{x} = \\{x_1, \\dots, x_{n^2}\\}$, the model starts generating pixels from the top left corner, from left to right and top to bottom (See Fig. 6).\nThe context for generating one pixel in PixelRNN. (Image source: Oord et al, 2016) Every pixel $x_i$ is sampled from a probability distribution conditional over the the past context: pixels above it or on the left of it when in the same row. The definition of such context looks pretty arbitrary, because how visual attention is attended to an image is more flexible. Somehow magically a generative model with such a strong assumption works.\nOne implementation that could capture the entire context is the Diagonal BiLSTM. First, apply the skewing operation by offsetting each row of the input feature map by one position with respect to the previous row, so that computation for each row can be parallelized. Then the LSTM states are computed with respect to the current pixel and the pixels on the left.\n(a) PixelRNN with diagonal BiLSTM. (b) Skewing operation that offsets each row in the feature map by one with regards to the row above. (Image source: Oord et al, 2016) $$ \\begin{aligned} \\lbrack \\mathbf{o}_i, \\mathbf{f}_i, \\mathbf{i}_i, \\mathbf{g}_i \\rbrack \u0026= \\sigma(\\mathbf{K}^{ss} \\circledast \\mathbf{h}_{i-1} + \\mathbf{K}^{is} \\circledast \\mathbf{x}_i) \u0026 \\scriptstyle{\\text{; }\\sigma\\scriptstyle{\\text{ is tanh for g, but otherwise sigmoid; }}\\circledast\\scriptstyle{\\text{ is convolution operation.}}} \\\\ \\mathbf{c}_i \u0026= \\mathbf{f}_i \\odot \\mathbf{c}_{i-1} + \\mathbf{i}_i \\odot \\mathbf{g}_i \u0026 \\scriptstyle{\\text{; }}\\odot\\scriptstyle{\\text{ is elementwise product.}}\\\\ \\mathbf{h}_i \u0026= \\mathbf{o}_i \\odot \\tanh(\\mathbf{c}_i) \\end{aligned} $$ where $\\circledast$ denotes the convolution operation and $\\odot$ is the element-wise multiplication. The input-to-state component $\\mathbf{K}^{is}$ is a 1x1 convolution, while the state-to-state recurrent component is computed with a column-wise convolution $\\mathbf{K}^{ss}$ with a kernel of size 2x1.\nThe diagonal BiLSTM layers are capable of processing an unbounded context field, but expensive to compute due to the sequential dependency between states. A faster implementation uses multiple convolutional layers without pooling to define a bounded context box. The convolution kernel is masked so that the future context is not seen, similar to MADE. This convolution version is called PixelCNN.\nPixelCNN with masked convolution constructed by an elementwise product of a mask tensor and the convolution kernel before applying it. (Image source: http://slazebni.cs.illinois.edu/spring17/lec13_advanced.pdf) WaveNet WaveNet (Van Den Oord, et al. 2016) is very similar to PixelCNN but applied to 1-D audio signals. WaveNet consists of a stack of causal convolution which is a convolution operation designed to respect the ordering: the prediction at a certain timestamp can only consume the data observed in the past, no dependency on the future. In PixelCNN, the causal convolution is implemented by masked convolution kernel. The causal convolution in WaveNet is simply to shift the output by a number of timestamps to the future so that the output is aligned with the last input element.\nOne big drawback of convolution layer is a very limited size of receptive field. The output can hardly depend on the input hundreds or thousands of timesteps ago, which can be a crucial requirement for modeling long sequences. WaveNet therefore adopts dilated convolution (animation), where the kernel is applied to an evenly-distributed subset of samples in a much larger receptive field of the input.\nVisualization of WaveNet models with a stack of (top) causal convolution layers and (bottom) dilated convolution layers. (Image source: Van Den Oord, et al. 2016) WaveNet uses the gated activation unit as the non-linear layer, as it is found to work significantly better than ReLU for modeling 1-D audio data. The residual connection is applied after the gated activation.\n$$ \\mathbf{z} = \\tanh(\\mathbf{W}_{f,k}\\circledast\\mathbf{x})\\odot\\sigma(\\mathbf{W}_{g,k}\\circledast\\mathbf{x}) $$ where $\\mathbf{W}_{f,k}$ and $\\mathbf{W}_{g,k}$ are convolution filter and gate weight matrix of the $k$-th layer, respectively; both are learnable.\nMasked Autoregressive Flow Masked Autoregressive Flow (MAF; Papamakarios et al., 2017) is a type of normalizing flows, where the transformation layer is built as an autoregressive neural network. MAF is very similar to Inverse Autoregressive Flow (IAF) introduced later. See more discussion on the relationship between MAF and IAF in the next section.\nGiven two random variables, $\\mathbf{z} \\sim \\pi(\\mathbf{z})$ and $\\mathbf{x} \\sim p(\\mathbf{x})$ and the probability density function $\\pi(\\mathbf{z})$ is known, MAF aims to learn $p(\\mathbf{x})$. MAF generates each $x_i$ conditioned on the past dimensions $\\mathbf{x}_{1:i-1}$.\nPrecisely the conditional probability is an affine transformation of $\\mathbf{z}$, where the scale and shift terms are functions of the observed part of $\\mathbf{x}$.\nData generation, producing a new $\\mathbf{x}$: $x_i \\sim p(x_i\\vert\\mathbf{x}_{1:i-1}) = z_i \\odot \\sigma_i(\\mathbf{x}_{1:i-1}) + \\mu_i(\\mathbf{x}_{1:i-1})\\text{, where }\\mathbf{z} \\sim \\pi(\\mathbf{z})$\nDensity estimation, given a known $\\mathbf{x}$: $p(\\mathbf{x}) = \\prod_{i=1}^D p(x_i\\vert\\mathbf{x}_{1:i-1})$\nThe generation procedure is sequential, so it is slow by design. While density estimation only needs one pass the network using architecture like MADE. The transformation function is trivial to inverse and the Jacobian determinant is easy to compute too.\nInverse Autoregressive Flow Similar to MAF, Inverse autoregressive flow (IAF; Kingma et al., 2016) models the conditional probability of the target variable as an autoregressive model too, but with a reversed flow, thus achieving a much efficient sampling process.\nFirst, let’s reverse the affine transformation in MAF:\n$$ z_i = \\frac{x_i - \\mu_i(\\mathbf{x}_{1:i-1})}{\\sigma_i(\\mathbf{x}_{1:i-1})} = -\\frac{\\mu_i(\\mathbf{x}_{1:i-1})}{\\sigma_i(\\mathbf{x}_{1:i-1})} + x_i \\odot \\frac{1}{\\sigma_i(\\mathbf{x}_{1:i-1})} $$ If let:\n$$ \\begin{aligned} \u0026 \\tilde{\\mathbf{x}} = \\mathbf{z}\\text{, }\\tilde{p}(.) = \\pi(.)\\text{, }\\tilde{\\mathbf{x}} \\sim \\tilde{p}(\\tilde{\\mathbf{x}}) \\\\ \u0026 \\tilde{\\mathbf{z}} = \\mathbf{x} \\text{, }\\tilde{\\pi}(.) = p(.)\\text{, }\\tilde{\\mathbf{z}} \\sim \\tilde{\\pi}(\\tilde{\\mathbf{z}})\\\\ \u0026 \\tilde{\\mu}_i(\\tilde{\\mathbf{z}}_{1:i-1}) = \\tilde{\\mu}_i(\\mathbf{x}_{1:i-1}) = -\\frac{\\mu_i(\\mathbf{x}_{1:i-1})}{\\sigma_i(\\mathbf{x}_{1:i-1})} \\\\ \u0026 \\tilde{\\sigma}(\\tilde{\\mathbf{z}}_{1:i-1}) = \\tilde{\\sigma}(\\mathbf{x}_{1:i-1}) = \\frac{1}{\\sigma_i(\\mathbf{x}_{1:i-1})} \\end{aligned} $$ Then we would have,\n$$ \\tilde{x}_i \\sim p(\\tilde{x}_i\\vert\\tilde{\\mathbf{z}}_{1:i}) = \\tilde{z}_i \\odot \\tilde{\\sigma}_i(\\tilde{\\mathbf{z}}_{1:i-1}) + \\tilde{\\mu}_i(\\tilde{\\mathbf{z}}_{1:i-1}) \\text{, where }\\tilde{\\mathbf{z}} \\sim \\tilde{\\pi}(\\tilde{\\mathbf{z}}) $$ IAF intends to estimate the probability density function of $\\tilde{\\mathbf{x}}$ given that $\\tilde{\\pi}(\\tilde{\\mathbf{z}})$ is already known. The inverse flow is an autoregressive affine transformation too, same as in MAF, but the scale and shift terms are autoregressive functions of observed variables from the known distribution $\\tilde{\\pi}(\\tilde{\\mathbf{z}})$. See the comparison between MAF and IAF in Comparison of MAF and IAF. The variable with known density is in green while the unknown one is in red.\nComputations of the individual elements $\\tilde{x}_i$ do not depend on each other, so they are easily parallelizable (only one pass using MADE). The density estimation for a known $\\tilde{\\mathbf{x}}$ is not efficient, because we have to recover the value of $\\tilde{z}_i$ in a sequential order, $\\tilde{z}_i = (\\tilde{x}_i - \\tilde{\\mu}_i(\\tilde{\\mathbf{z}}_{1:i-1})) / \\tilde{\\sigma}_i(\\tilde{\\mathbf{z}}_{1:i-1})$, thus D times in total.\nBase distribution Target distribution Model Data generation Density estimation MAF $\\mathbf{z}\\sim\\pi(\\mathbf{z})$ $\\mathbf{x}\\sim p(\\mathbf{x})$ $x_i = z_i \\odot \\sigma_i(\\mathbf{x}_{1:i-1}) + \\mu_i(\\mathbf{x}_{1:i-1})$ Sequential; slow One pass; fast IAF $\\tilde{\\mathbf{z}}\\sim\\tilde{\\pi}(\\tilde{\\mathbf{z}})$ $\\tilde{\\mathbf{x}}\\sim\\tilde{p}(\\tilde{\\mathbf{x}})$ $\\tilde{x}_i = \\tilde{z}_i \\odot \\tilde{\\sigma}_i(\\tilde{\\mathbf{z}}_{1:i-1}) + \\tilde{\\mu}_i(\\tilde{\\mathbf{z}}_{1:i-1})$ One pass; fast Sequential; slow ———- ———- ———- ———- ———- ———- VAE + Flows In Variational Autoencoder, if we want to model the posterior $p(\\mathbf{z}\\vert\\mathbf{x})$ as a more complicated distribution rather than simple Gaussian. Intuitively we can use normalizing flow to transform the base Gaussian for better density approximation. The encoder then would predict a set of scale and shift terms $(\\mu_i, \\sigma_i)$ which are all functions of input $\\mathbf{x}$. Read the paper for more details if interested.\nIf you notice mistakes and errors in this post, don’t hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be very happy to correct them right away!\nSee you in the next post :D\nCited as:\n@article{weng2018flow, title = \"Flow-based Deep Generative Models\", author = \"Weng, Lilian\", journal = \"lilianweng.github.io\", year = \"2018\", url = \"https://lilianweng.github.io/posts/2018-10-13-flow-models/\" } Reference [1] Danilo Jimenez Rezende, and Shakir Mohamed. “Variational inference with normalizing flows.” ICML 2015.\n[2] Normalizing Flows Tutorial, Part 1: Distributions and Determinants by Eric Jang.\n[3] Normalizing Flows Tutorial, Part 2: Modern Normalizing Flows by Eric Jang.\n[4] Normalizing Flows by Adam Kosiorek.\n[5] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. “Density estimation using Real NVP.” ICLR 2017.\n[6] Laurent Dinh, David Krueger, and Yoshua Bengio. “NICE: Non-linear independent components estimation.” ICLR 2015 Workshop track.\n[7] Diederik P. Kingma, and Prafulla Dhariwal. “Glow: Generative flow with invertible 1x1 convolutions.” arXiv:1807.03039 (2018).\n[8] Germain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. “Made: Masked autoencoder for distribution estimation.” ICML 2015.\n[9] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. “Pixel recurrent neural networks.” ICML 2016.\n[10] Diederik P. Kingma, et al. “Improved variational inference with inverse autoregressive flow.” NIPS. 2016.\n[11] George Papamakarios, Iain Murray, and Theo Pavlakou. “Masked autoregressive flow for density estimation.” NIPS 2017.\n[12] Jianlin Su, and Guang Wu. “f-VAEs: Improve VAEs with Conditional Flows.” arXiv:1809.05861 (2018).\n[13] Van Den Oord, Aaron, et al. “WaveNet: A generative model for raw audio.” SSW. 2016.\n",
  "wordCount" : "4313",
  "inLanguage": "en",
  "datePublished": "2018-10-13T00:00:00Z",
  "dateModified": "2018-10-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lilian Weng"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lilianweng.github.io/posts/2018-10-13-flow-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lil'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lilianweng.github.io/favicon_wine.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lilianweng.github.io/" accesskey="h" title="Lil&#39;Log (Alt + H)">Lil&#39;Log</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lilianweng.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Flow-based Deep Generative Models
    </h1>
    <div class="post-meta">Date: October 13, 2018  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#types-of-generative-models" aria-label="Types of Generative Models">Types of Generative Models</a></li>
                <li>
                    <a href="#linear-algebra-basics-recap" aria-label="Linear Algebra Basics Recap">Linear Algebra Basics Recap</a><ul>
                        
                <li>
                    <a href="#jacobian-matrix-and-determinant" aria-label="Jacobian Matrix and Determinant">Jacobian Matrix and Determinant</a></li>
                <li>
                    <a href="#change-of-variable-theorem" aria-label="Change of Variable Theorem">Change of Variable Theorem</a></li></ul>
                </li>
                <li>
                    <a href="#what-is-normalizing-flows" aria-label="What is Normalizing Flows?">What is Normalizing Flows?</a></li>
                <li>
                    <a href="#models-with-normalizing-flows" aria-label="Models with Normalizing Flows">Models with Normalizing Flows</a><ul>
                        
                <li>
                    <a href="#realnvp" aria-label="RealNVP">RealNVP</a></li>
                <li>
                    <a href="#nice" aria-label="NICE">NICE</a></li>
                <li>
                    <a href="#glow" aria-label="Glow">Glow</a></li></ul>
                </li>
                <li>
                    <a href="#models-with-autoregressive-flows" aria-label="Models with Autoregressive Flows">Models with Autoregressive Flows</a><ul>
                        
                <li>
                    <a href="#made" aria-label="MADE">MADE</a></li>
                <li>
                    <a href="#pixelrnn" aria-label="PixelRNN">PixelRNN</a></li>
                <li>
                    <a href="#wavenet" aria-label="WaveNet">WaveNet</a></li>
                <li>
                    <a href="#masked-autoregressive-flow" aria-label="Masked Autoregressive Flow">Masked Autoregressive Flow</a></li>
                <li>
                    <a href="#inverse-autoregressive-flow" aria-label="Inverse Autoregressive Flow">Inverse Autoregressive Flow</a></li></ul>
                </li>
                <li>
                    <a href="#vae--flows" aria-label="VAE &#43; Flows">VAE + Flows</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><!-- In this post, we are looking into the third type of generative models: flow-based generative models. Different from GAN and VAE, they explicitly learn the probability density function of the input data. -->
<p>So far, I&rsquo;ve written about two types of generative models, <a href="https://lilianweng.github.io/posts/2017-08-20-gan/">GAN</a> and <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">VAE</a>. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) &mdash; because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$.</p>
<p>Flow-based deep generative models conquer this hard problem with the help of <a href="https://arxiv.org/abs/1505.05770">normalizing flows</a>, a powerful statistics tool for density estimation. A good estimation of $p(\mathbf{x})$ makes it possible to efficiently complete many downstream tasks: sample unobserved but realistic new data points (data generation), predict the rareness of future events (density estimation), infer latent variables, fill in incomplete data samples, etc.</p>
<h1 id="types-of-generative-models">Types of Generative Models<a hidden class="anchor" aria-hidden="true" href="#types-of-generative-models">#</a></h1>
<p>Here is a quick summary of the difference between GAN, VAE, and flow-based generative models:</p>
<ol>
<li>Generative adversarial networks: GAN provides a smart solution to model the data generation, an unsupervised learning problem, as a supervised one. The discriminator model learns to distinguish the real data from the fake samples that are produced by the generator model. Two models are trained as they are playing a <a href="https://en.wikipedia.org/wiki/Minimax">minimax</a> game.</li>
<li>Variational autoencoders: VAE inexplicitly optimizes the log-likelihood of the data by maximizing the evidence lower bound (ELBO).</li>
<li>Flow-based generative models: A flow-based generative model is constructed by a sequence of invertible transformations. Unlike other two, the model explicitly learns the data distribution $p(\mathbf{x})$ and therefore the loss function is simply the negative log-likelihood.</li>
</ol>
<figure>
	<img src="three-generative-models.png" style="width: 100%;"  />
	<figcaption>Comparison of three categories of generative models.</figcaption>
</figure>
<h1 id="linear-algebra-basics-recap">Linear Algebra Basics Recap<a hidden class="anchor" aria-hidden="true" href="#linear-algebra-basics-recap">#</a></h1>
<p>We should understand two key concepts before getting into the flow-based generative model: the Jacobian determinant and the change of variable rule. Pretty basic, so feel free to skip.</p>
<h2 id="jacobian-matrix-and-determinant">Jacobian Matrix and Determinant<a hidden class="anchor" aria-hidden="true" href="#jacobian-matrix-and-determinant">#</a></h2>
<p>Given a function of mapping a $n$-dimensional input vector $\mathbf{x}$ to a $m$-dimensional output vector, $\mathbf{f}: \mathbb{R}^n \mapsto \mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the <strong>Jacobian matrix</strong>, $\mathbf{J}$ where one entry on the i-th row and j-th column is $\mathbf{J}_{ij} = \frac{\partial f_i}{\partial x_j}$.</p>
<div>
$$
\mathbf{J} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_n} \\[6pt]
\vdots & \ddots & \vdots \\[6pt]
\frac{\partial f_m}{\partial x_1} & \dots & \frac{\partial f_m}{\partial x_n} \\[6pt]
\end{bmatrix}
$$
</div>
<p>The determinant is one real number computed as a function of all the elements in a squared matrix. Note that the determinant <em>only exists for <strong>square</strong> matrices</em>. The absolute value of the determinant can be thought of as a measure of <em>&ldquo;how much multiplication by the matrix expands or contracts space&rdquo;.</em></p>
<p>The determinant of a nxn matrix $M$ is:</p>
<div>
$$
\det M = \det \begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n} \\
\vdots & \vdots & & \vdots \\
a_{n1} & a_{n2} & \dots & a_{nn} \\
\end{bmatrix} = \sum_{j_1 j_2 \dots j_n} (-1)^{\tau(j_1 j_2 \dots j_n)} a_{1j_1} a_{2j_2} \dots a_{nj_n}
$$
</div>
<p>where the subscript under the summation $j_1 j_2 \dots j_n$ are all permutations of the set {1, 2, &hellip;, n}, so there are $n!$ items in total; $\tau(.)$ indicates the <a href="https://en.wikipedia.org/wiki/Parity_of_a_permutation">signature</a> of a permutation.</p>
<p>The determinant of a square matrix $M$ detects whether it is invertible: If $\det(M)=0$ then $M$ is not invertible (a <em>singular</em> matrix with linearly dependent rows or columns; or any row or column is all 0); otherwise, if $\det(M)\neq 0$, $M$ is invertible.</p>
<p>The determinant of the product is equivalent to the product of the determinants: $\det(AB) = \det(A)\det(B)$. (<a href="https://proofwiki.org/wiki/Determinant_of_Matrix_Product">proof</a>)</p>
<h2 id="change-of-variable-theorem">Change of Variable Theorem<a hidden class="anchor" aria-hidden="true" href="#change-of-variable-theorem">#</a></h2>
<p>Let&rsquo;s review the change of variable theorem specifically in the context of probability density estimation, starting with a single variable case.</p>
<p>Given a random variable $z$ and its known probability density function $z \sim \pi(z)$, we would like to construct a new random variable using a 1-1 mapping function $x = f(z)$. The function $f$ is invertible, so $z=f^{-1}(x)$. Now the question is <em>how to infer the unknown probability density function of the new variable</em>, $p(x)$?</p>
<div>
$$
\begin{aligned}
& \int p(x)dx = \int \pi(z)dz = 1 \scriptstyle{\text{   ; Definition of probability distribution.}}\\
& p(x) = \pi(z) \left\vert\frac{dz}{dx}\right\vert = \pi(f^{-1}(x)) \left\vert\frac{d f^{-1}}{dx}\right\vert = \pi(f^{-1}(x)) \vert (f^{-1})'(x) \vert
\end{aligned}
$$
</div>
<p>By definition, the integral $\int \pi(z)dz$ is the sum of an infinite number of rectangles of infinitesimal width $\Delta z$. The height of such a rectangle at position $z$ is the value of the density function $\pi(z)$. When we substitute the variable, $z = f^{-1}(x)$ yields $\frac{\Delta z}{\Delta x} = (f^{-1}(x))&rsquo;$ and $\Delta z =  (f^{-1}(x))&rsquo; \Delta x$. Here $\vert(f^{-1}(x))&rsquo;\vert$ indicates the ratio between the area of rectangles defined in two different coordinate of variables $z$ and $x$ respectively.</p>
<p>The multivariable version has a similar format:</p>
<div>
$$
\begin{aligned}
\mathbf{z} &\sim \pi(\mathbf{z}), \mathbf{x} = f(\mathbf{z}), \mathbf{z} = f^{-1}(\mathbf{x}) \\
p(\mathbf{x}) 
&= \pi(\mathbf{z}) \left\vert \det \dfrac{d \mathbf{z}}{d \mathbf{x}} \right\vert  
= \pi(f^{-1}(\mathbf{x})) \left\vert \det \dfrac{d f^{-1}}{d \mathbf{x}} \right\vert
\end{aligned}
$$
</div>
<p>where $\det \frac{\partial f}{\partial\mathbf{z}}$ is the Jacobian determinant of the function $f$. The full proof of the multivariate version is out of the scope of this post; ask Google if interested ;)</p>
<h1 id="what-is-normalizing-flows">What is Normalizing Flows?<a hidden class="anchor" aria-hidden="true" href="#what-is-normalizing-flows">#</a></h1>
<p>Being able to do good density estimation has direct applications in many machine learning problems, but it is very hard. For example, since we need to run backward propagation in deep learning models, the embedded probability distribution (i.e. posterior $p(\mathbf{z}\vert\mathbf{x})$) is expected to be simple enough to calculate the derivative easily and efficiently. That is why Gaussian distribution is often used in latent variable generative models, even though most of real world distributions are much more complicated than Gaussian.</p>
<p>Here comes a <strong>Normalizing Flow</strong> (NF) model for better and more powerful distribution approximation. A normalizing flow transforms a simple distribution into a complex one by applying a sequence of invertible transformation functions. Flowing through a chain of transformations, we repeatedly substitute the variable for the new one according to the change of variables theorem and eventually obtain a probability distribution of the final target variable.</p>
<figure>
	<img src="normalizing-flow.png" style="width: 100%;"  />
	<figcaption>Illustration of a normalizing flow model, transforming a simple distribution $p\_0(\mathbf{z}\_0)$ to a complex one $p\_K(\mathbf{z}\_K)$ step by step.</figcaption>
</figure>
<p>As defined in Fig. 2,</p>
<div>
$$
\begin{aligned}
\mathbf{z}_{i-1} &\sim p_{i-1}(\mathbf{z}_{i-1}) \\
\mathbf{z}_i &= f_i(\mathbf{z}_{i-1})\text{, thus }\mathbf{z}_{i-1} = f_i^{-1}(\mathbf{z}_i) \\
p_i(\mathbf{z}_i) 
&= p_{i-1}(f_i^{-1}(\mathbf{z}_i)) \left\vert \det\dfrac{d f_i^{-1}}{d \mathbf{z}_i} \right\vert
\end{aligned}
$$
</div>
<p>Then let&rsquo;s convert the equation to be a function of $\mathbf{z}_i$ so that we can do inference with the base distribution.</p>
<div>
$$
\begin{aligned}
p_i(\mathbf{z}_i) 
&= p_{i-1}(f_i^{-1}(\mathbf{z}_i)) \left\vert \det\dfrac{d f_i^{-1}}{d \mathbf{z}_i} \right\vert \\
&= p_{i-1}(\mathbf{z}_{i-1}) \left\vert \det \color{red}{\Big(\dfrac{d f_i}{d\mathbf{z}_{i-1}}\Big)^{-1}} \right\vert & \scriptstyle{\text{; According to the inverse func theorem.}} \\
&= p_{i-1}(\mathbf{z}_{i-1}) \color{red}{\left\vert \det \dfrac{d f_i}{d\mathbf{z}_{i-1}} \right\vert^{-1}} & \scriptstyle{\text{; According to a property of Jacobians of invertible func.}} \\
\log p_i(\mathbf{z}_i) &= \log p_{i-1}(\mathbf{z}_{i-1}) - \log \left\vert \det \dfrac{d f_i}{d\mathbf{z}_{i-1}} \right\vert
\end{aligned}
$$
</div>
<p>(*) A note on the <em>&ldquo;inverse function theorem&rdquo;</em>: If $y=f(x)$ and $x=f^{-1}(y)$, we have:</p>
<div>
$$
\dfrac{df^{-1}(y)}{dy} = \dfrac{dx}{dy} = (\dfrac{dy}{dx})^{-1} = (\dfrac{df(x)}{dx})^{-1}
$$
</div>
<p>(*) A note on <em>&ldquo;Jacobians of invertible function&rdquo;</em>: The determinant of the inverse of an invertible matrix is the inverse of the determinant: $\det(M^{-1}) = (\det(M))^{-1}$, <a href="#jacobian-matrix-and-determinant">because</a> $\det(M)\det(M^{-1}) = \det(M \cdot M^{-1}) = \det(I) = 1$.</p>
<p>Given such a chain of probability density functions, we know the relationship between each pair of consecutive variables. We can expand the equation of the output $\mathbf{x}$ step by step until tracing back to the initial distribution $\mathbf{z}_0$.</p>
<div>
$$
\begin{aligned}
\mathbf{x} = \mathbf{z}_K &= f_K \circ f_{K-1} \circ \dots \circ f_1 (\mathbf{z}_0) \\
\log p(\mathbf{x}) = \log \pi_K(\mathbf{z}_K) 
&= \log \pi_{K-1}(\mathbf{z}_{K-1}) - \log\left\vert\det\dfrac{d f_K}{d \mathbf{z}_{K-1}}\right\vert \\
&= \log \pi_{K-2}(\mathbf{z}_{K-2}) - \log\left\vert\det\dfrac{d f_{K-1}}{d\mathbf{z}_{K-2}}\right\vert - \log\left\vert\det\dfrac{d f_K}{d\mathbf{z}_{K-1}}\right\vert \\
&= \dots \\
&= \log \pi_0(\mathbf{z}_0) - \sum_{i=1}^K \log\left\vert\det\dfrac{d f_i}{d\mathbf{z}_{i-1}}\right\vert
\end{aligned}
$$
</div>
<p>The path traversed by the random variables $\mathbf{z}_i = f_i(\mathbf{z}_{i-1})$ is the <strong>flow</strong> and the full chain formed by the successive distributions $\pi_i$ is called a <strong>normalizing flow</strong>. Required by the computation in the equation, a transformation function $f_i$ should satisfy two properties:</p>
<ol>
<li>It is easily invertible.</li>
<li>Its Jacobian determinant is easy to compute.</li>
</ol>
<h1 id="models-with-normalizing-flows">Models with Normalizing Flows<a hidden class="anchor" aria-hidden="true" href="#models-with-normalizing-flows">#</a></h1>
<p>With normalizing flows in our toolbox, the exact log-likelihood of input data $\log p(\mathbf{x})$ becomes tractable. As a result, the training criterion of flow-based generative model is simply the negative log-likelihood (NLL) over the training dataset $\mathcal{D}$:</p>
<div>
$$
\mathcal{L}(\mathcal{D}) = - \frac{1}{\vert\mathcal{D}\vert}\sum_{\mathbf{x} \in \mathcal{D}} \log p(\mathbf{x})
$$
</div>
<h2 id="realnvp">RealNVP<a hidden class="anchor" aria-hidden="true" href="#realnvp">#</a></h2>
<p>The <strong>RealNVP</strong> (Real-valued Non-Volume Preserving; <a href="https://arxiv.org/abs/1605.08803">Dinh et al., 2017</a>) model implements a normalizing flow by stacking a sequence of invertible bijective transformation functions. In each bijection $f: \mathbf{x} \mapsto \mathbf{y}$, known as <em>affine coupling layer</em>,  the input dimensions are split into two parts:</p>
<ul>
<li>The first $d$ dimensions stay same;</li>
<li>The second part, $d+1$ to $D$ dimensions, undergo an affine transformation (&ldquo;scale-and-shift&rdquo;) and both the scale and shift parameters are functions of the first $d$ dimensions.</li>
</ul>
<div>
$$
\begin{aligned}
\mathbf{y}_{1:d} &= \mathbf{x}_{1:d} \\ 
\mathbf{y}_{d+1:D} &= \mathbf{x}_{d+1:D} \odot \exp({s(\mathbf{x}_{1:d})}) + t(\mathbf{x}_{1:d})
\end{aligned}
$$
</div>
<p>where $s(.)$ and $t(.)$ are <em>scale</em> and <em>translation</em> functions and both map $\mathbb{R}^d \mapsto \mathbb{R}^{D-d}$. The $\odot$ operation is the element-wise product.</p>
<p>Now let&rsquo;s check whether this transformation satisfy two basic properties  for a flow transformation.</p>
<p><strong>Condition 1</strong>: &ldquo;It is easily invertible.&rdquo;</p>
<p>Yes and it is fairly straightforward.</p>
<div>
$$
\begin{cases}
\mathbf{y}_{1:d} &= \mathbf{x}_{1:d} \\ 
\mathbf{y}_{d+1:D} &= \mathbf{x}_{d+1:D} \odot \exp({s(\mathbf{x}_{1:d})}) + t(\mathbf{x}_{1:d})
\end{cases}
\Leftrightarrow 
\begin{cases}
\mathbf{x}_{1:d} &= \mathbf{y}_{1:d} \\ 
\mathbf{x}_{d+1:D} &= (\mathbf{y}_{d+1:D} - t(\mathbf{y}_{1:d})) \odot \exp(-s(\mathbf{y}_{1:d}))
\end{cases}
$$
</div>
<p><strong>Condition 2</strong>: &ldquo;Its Jacobian determinant is easy to compute.&rdquo;</p>
<p>Yes. It is not hard to get the Jacobian matrix and determinant of this transformation. The Jacobian is a lower triangular matrix.</p>
<div>
$$
\mathbf{J} = 
\begin{bmatrix}
  \mathbb{I}_d & \mathbf{0}_{d\times(D-d)} \\[5pt]
  \frac{\partial \mathbf{y}_{d+1:D}}{\partial \mathbf{x}_{1:d}} & \text{diag}(\exp(s(\mathbf{x}_{1:d})))
\end{bmatrix}
$$
</div>
<p>Hence the determinant is simply the product of terms on the diagonal.</p>
<div>
$$
\det(\mathbf{J}) 
= \prod_{j=1}^{D-d}\exp(s(\mathbf{x}_{1:d}))_j
= \exp(\sum_{j=1}^{D-d} s(\mathbf{x}_{1:d})_j)
$$
</div>
<p>So far, the affine coupling layer looks perfect for constructing a normalizing flow :)</p>
<p>Even better, since (i) computing $f^{-1}$ does not require computing the inverse of $s$ or $t$ and (ii) computing the Jacobian determinant does not involve computing the Jacobian of $s$ or $t$, those functions can be <em>arbitrarily complex</em>; i.e. both $s$ and $t$ can be modeled by deep neural networks.</p>
<p>In one affine coupling layer, some dimensions (channels) remain unchanged. To make sure all the inputs have a chance to be altered, the model reverses the ordering in each layer so that different components are left unchanged. Following such an alternating pattern, the set of units which remain identical in one transformation layer are always modified in the next. Batch normalization is found to help training models with a very deep stack of coupling layers.</p>
<p>Furthermore, RealNVP can work in a multi-scale architecture to build a more efficient model for large inputs. The multi-scale architecture applies several &ldquo;sampling&rdquo; operations to normal affine layers, including spatial checkerboard pattern masking, squeezing operation, and channel-wise masking. Read the <a href="https://arxiv.org/abs/1605.08803">paper</a> for more details on the multi-scale architecture.</p>
<h2 id="nice">NICE<a hidden class="anchor" aria-hidden="true" href="#nice">#</a></h2>
<p>The <strong>NICE</strong> (Non-linear Independent Component Estimation; <a href="https://arxiv.org/abs/1410.8516">Dinh, et al. 2015</a>) model is a predecessor of <a href="#realnvp">RealNVP</a>. The transformation in NICE is the affine coupling layer without the scale term, known as <em>additive coupling layer</em>.</p>
<div>
$$
\begin{cases}
\mathbf{y}_{1:d} &= \mathbf{x}_{1:d} \\ 
\mathbf{y}_{d+1:D} &= \mathbf{x}_{d+1:D} + m(\mathbf{x}_{1:d})
\end{cases}
\Leftrightarrow 
\begin{cases}
\mathbf{x}_{1:d} &= \mathbf{y}_{1:d} \\ 
\mathbf{x}_{d+1:D} &= \mathbf{y}_{d+1:D} - m(\mathbf{y}_{1:d})
\end{cases}
$$
</div>
<h2 id="glow">Glow<a hidden class="anchor" aria-hidden="true" href="#glow">#</a></h2>
<p>The <strong>Glow</strong> (<a href="https://arxiv.org/abs/1807.03039">Kingma and Dhariwal, 2018</a>) model extends the previous reversible generative models, NICE and RealNVP, and simplifies the architecture by replacing the reverse permutation operation on the channel ordering with invertible 1x1 convolutions.</p>
<figure>
	<img src="one-glow-step.png" style="width: 45%;"  />
	<figcaption>One step of flow in the Glow model. (Image source: <a href="https://arxiv.org/abs/1807.03039" target="_blank">Kingma and Dhariwal, 2018</a>)</figcaption>
</figure>
<p>There are three substeps in one step of flow in Glow.</p>
<p>Substep 1: <strong>Activation normalization</strong> (short for &ldquo;actnorm&rdquo;)</p>
<p>It performs an affine transformation using a scale and bias parameter per channel, similar to batch normalization, but works for mini-batch size 1. The parameters are trainable but initialized so that the first minibatch of data have mean 0 and standard deviation 1 after actnorm.</p>
<p>Substep 2: <strong>Invertible 1x1 conv</strong></p>
<p>Between layers of the RealNVP flow, the ordering of channels is reversed so that all the data dimensions have a chance to be altered. A 1×1 convolution with equal number of input and output channels is <em>a generalization of any permutation</em> of the channel ordering.</p>
<p>Say, we have an invertible 1x1 convolution of an input $h \times w \times c$ tensor $\mathbf{h}$ with a weight matrix $\mathbf{W}$ of size $c \times c$. The output is a $h \times w \times c$ tensor, labeled as $f(\mathbf{h}) = \texttt{conv2d}(\mathbf{h}; \mathbf{W})$. In order to apply the change of variable rule, we need to compute the Jacobian determinant $\vert \det\partial f / \partial\mathbf{h}\vert$.</p>
<p>Both the input and output of 1x1 convolution here can be viewed as a matrix of size $h \times w$. Each entry $\mathbf{x}_{ij}$ ($i=1,\dots,h, j=1,\dots,w$) in $\mathbf{h}$ is a vector of $c$ channels and each entry is multiplied by the weight matrix $\mathbf{W}$ to obtain the corresponding entry $\mathbf{y}_{ij}$ in the output matrix respectively. The derivative of each entry is $\partial \mathbf{x}_{ij} \mathbf{W} / \partial\mathbf{x}_{ij} = \mathbf{W}$ and there are $h \times w$ such entries in total:</p>
<div>
$$
\log \left\vert\det \frac{\partial\texttt{conv2d}(\mathbf{h}; \mathbf{W})}{\partial\mathbf{h}}\right\vert
= \log (\vert\det\mathbf{W}\vert^{h \cdot w}\vert) = h \cdot w \cdot \log \vert\det\mathbf{W}\vert
$$
</div>
<p>The inverse 1x1 convolution depends on the inverse matrix $\mathbf{W}^{-1}$. Since the weight matrix is relatively small, the amount of computation for the matrix determinant (<a href="https://www.tensorflow.org/api_docs/python/tf/linalg/det">tf.linalg.det</a>) and inversion (<a href="https://www.tensorflow.org/api_docs/python/tf/linalg/inv">tf.linalg.inv</a>) is still under control.</p>
<p>Substep 3: <strong>Affine coupling layer</strong></p>
<p>The design is same as in RealNVP.</p>
<figure>
	<img src="glow-table.png" style="width: 100%;"  />
	<figcaption>Three substeps in one step of flow in Glow. (Image source: <a href="https://arxiv.org/abs/1807.03039" target="_blank">Kingma and Dhariwal, 2018</a>)</figcaption>
</figure>
<h1 id="models-with-autoregressive-flows">Models with Autoregressive Flows<a hidden class="anchor" aria-hidden="true" href="#models-with-autoregressive-flows">#</a></h1>
<p>The <strong>autoregressive</strong> constraint is a way to model sequential data, $\mathbf{x} = [x_1, \dots, x_D]$: each output only depends on the data observed in the past, but not on the future ones. In other words, the probability of observing $x_i$ is conditioned on $x_1, \dots, x_{i-1}$ and the product of these conditional probabilities gives us the probability of observing the full sequence:</p>
<div>
$$
p(\mathbf{x}) = \prod_{i=1}^{D} p(x_i\vert x_1, \dots, x_{i-1}) = \prod_{i=1}^{D} p(x_i\vert x_{1:i-1})
$$
</div>
<p>How to model the conditional density is of your choice. It can be a univariate Gaussian with mean and standard deviation computed as a function of $x_{1:i-1}$, or a multilayer neural network with $x_{1:i-1}$ as the input.</p>
<p>If a flow transformation in a normalizing flow is framed as an autoregressive model &mdash; each dimension in a vector variable is conditioned on the previous dimensions &mdash; this is an <strong>autoregressive flow</strong>.</p>
<p>This section starts with several classic autoregressive models (MADE, PixelRNN, WaveNet) and then we dive into autoregressive flow models (MAF and IAF).</p>
<h2 id="made">MADE<a hidden class="anchor" aria-hidden="true" href="#made">#</a></h2>
<p><strong>MADE</strong> (Masked Autoencoder for Distribution Estimation; <a href="https://arxiv.org/abs/1502.03509">Germain et al., 2015</a>) is a specially designed architecture to enforce the autoregressive property in the autoencoder <em>efficiently</em>. When using an autoencoder to predict the conditional probabilities, rather than feeding the autoencoder with input of different observation windows $D$ times, MADE removes the contribution from certain hidden units by multiplying binary mask matrices so that each input dimension is reconstructed only from previous dimensions in a <em>given</em> ordering in a <em>single pass</em>.</p>
<p>In a multilayer fully-connected neural network, say, we have $L$ hidden layers with weight matrices $\mathbf{W}^1, \dots, \mathbf{W}^L$ and an output layer with weight matrix $\mathbf{V}$. The output $\hat{\mathbf{x}}$ has each dimension $\hat{x}_i = p(x_i\vert x_{1:i-1})$.</p>
<p>Without any mask, the computation through layers looks like the following:</p>
<div>
$$
\begin{aligned}
\mathbf{h}^0 &= \mathbf{x} \\
\mathbf{h}^l &= \text{activation}^l(\mathbf{W}^l\mathbf{h}^{l-1} + \mathbf{b}^l) \\
\hat{\mathbf{x}} &= \sigma(\mathbf{V}\mathbf{h}^L + \mathbf{c})
\end{aligned}
$$
</div>
<figure>
	<img src="MADE.png" style="width: 75%;"  />
	<figcaption>Demonstration of how MADE works in a three-layer feed-forward neural network. (Image source: <a href="https://arxiv.org/abs/1502.03509" target="_blank">Germain et al., 2015</a>)</figcaption>
</figure>
<p>To zero out some connections between layers, we can simply element-wise multiply every weight matrix by a binary mask matrix. Each hidden node is assigned with a random &ldquo;connectivity integer&rdquo; between $1$ and $D-1$; the assigned value for the $k$-th unit in the $l$-th layer is denoted by $m^l_k$. The binary mask matrix is determined by element-wise comparing values of two nodes in two layers.</p>
<div>
$$
\begin{aligned}
\mathbf{h}^l &= \text{activation}^l((\mathbf{W}^l \color{red}{\odot \mathbf{M}^{\mathbf{W}^l}}) \mathbf{h}^{l-1} + \mathbf{b}^l) \\
\hat{\mathbf{x}} &= \sigma((\mathbf{V} \color{red}{\odot \mathbf{M}^{\mathbf{V}}}) \mathbf{h}^L + \mathbf{c}) \\
M^{\mathbf{W}^l}_{k', k} 
&= \mathbf{1}_{m^l_{k'} \geq m^{l-1}_k} 
= \begin{cases}
    1, & \text{if } m^l_{k'} \geq m^{l-1}_k\\
    0, & \text{otherwise}
\end{cases} \\
M^{\mathbf{V}}_{d, k} 
&= \mathbf{1}_{d \geq m^L_k} 
= \begin{cases}
    1, & \text{if } d > m^L_k\\
    0, & \text{otherwise}
\end{cases}
\end{aligned}
$$
</div>
<p>A unit in the current layer can only be connected to other units with equal or smaller numbers in the previous layer and this type of dependency easily propagates through the network up to the output layer. Once the numbers are assigned to all the units and layers, the ordering of input dimensions is fixed and the conditional probability is produced with respect to it.  See a great illustration in To make sure all the hidden units are connected to the input and output layers through some paths, the $m^l_k$ is sampled to be equal or greater than the minimal connectivity integer in the previous layer, $\min_{k&rsquo;} m_{k&rsquo;}^{l-1}$.</p>
<p>MADE training can be further facilitated by:</p>
<ul>
<li><em>Order-agnostic training</em>: shuffle the input dimensions, so that MADE is able to model any arbitrary ordering; can create an ensemble of autoregressive models at the runtime.</li>
<li><em>Connectivity-agnostic training</em>: to avoid a model being tied up to a specific connectivity pattern constraints, resample $m^l_k$ for each training minibatch.</li>
</ul>
<h2 id="pixelrnn">PixelRNN<a hidden class="anchor" aria-hidden="true" href="#pixelrnn">#</a></h2>
<p>PixelRNN (<a href="https://arxiv.org/abs/1601.06759">Oord et al, 2016</a>) is a deep generative model for images. The image is generated one pixel at a time and each new pixel is sampled conditional on the pixels that have been seen before.</p>
<p>Let&rsquo;s consider an image of size $n \times n$, $\mathbf{x} = \{x_1, \dots, x_{n^2}\}$, the model starts generating pixels from the top left corner, from left to right and top to bottom (See Fig. 6).</p>
<figure>
	<img src="pixel-rnn-context.png" style="width: 30%;"  />
	<figcaption>The context for generating one pixel in PixelRNN. (Image source: <a href="https://arxiv.org/abs/1601.06759" target="_blank">Oord et al, 2016</a>)</figcaption>
</figure>
<p>Every pixel $x_i$ is sampled from a probability distribution conditional over the the past context: pixels above it or on the left of it when in the same row. The definition of such context looks pretty arbitrary, because how visual <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">attention</a> is attended to an image is more flexible. Somehow magically a generative model with such a strong assumption works.</p>
<p>One implementation that could capture the entire context is the <em>Diagonal BiLSTM</em>. First, apply the <strong>skewing</strong> operation by offsetting each row of the input feature map by one position with respect to the previous row, so that computation for each row can be parallelized. Then the LSTM states are computed with respect to the current pixel and the pixels on the left.</p>
<figure>
	<img src="diagonal-biLSTM.png" style="width: 100%;"  />
	<figcaption>(a) PixelRNN with diagonal BiLSTM. (b) Skewing operation that offsets each row in the feature map by one with regards to the row above. (Image source: <a href="https://arxiv.org/abs/1601.06759" target="_blank">Oord et al, 2016</a>)</figcaption>
</figure>
<div>
$$
\begin{aligned}
\lbrack \mathbf{o}_i, \mathbf{f}_i, \mathbf{i}_i, \mathbf{g}_i \rbrack &= \sigma(\mathbf{K}^{ss} \circledast \mathbf{h}_{i-1} + \mathbf{K}^{is} \circledast \mathbf{x}_i) & \scriptstyle{\text{; }\sigma\scriptstyle{\text{ is tanh for g, but otherwise sigmoid; }}\circledast\scriptstyle{\text{ is convolution operation.}}} \\
\mathbf{c}_i &= \mathbf{f}_i \odot \mathbf{c}_{i-1} + \mathbf{i}_i \odot \mathbf{g}_i & \scriptstyle{\text{; }}\odot\scriptstyle{\text{ is elementwise product.}}\\
\mathbf{h}_i &= \mathbf{o}_i \odot \tanh(\mathbf{c}_i)
\end{aligned}
$$
</div>
<p>where $\circledast$ denotes the convolution operation and $\odot$ is the element-wise multiplication. The input-to-state component $\mathbf{K}^{is}$ is a 1x1 convolution, while the state-to-state recurrent component is computed with a column-wise convolution $\mathbf{K}^{ss}$ with a kernel of size 2x1.</p>
<p>The diagonal BiLSTM layers are capable of processing an unbounded context field, but expensive to compute due to the sequential dependency between states. A faster implementation uses multiple convolutional layers without pooling to define a bounded context box. The convolution kernel is masked so that the future context is not seen, similar to <a href="#MADE">MADE</a>. This convolution version is called <strong>PixelCNN</strong>.</p>
<figure>
	<img src="pixel-cnn.png" style="width: 50%;"  />
	<figcaption>PixelCNN with masked convolution constructed by an elementwise product of a mask tensor and the convolution kernel before applying it. (Image source: http://slazebni.cs.illinois.edu/spring17/lec13_advanced.pdf)</figcaption>
</figure>
<h2 id="wavenet">WaveNet<a hidden class="anchor" aria-hidden="true" href="#wavenet">#</a></h2>
<p><strong>WaveNet</strong> (<a href="https://arxiv.org/abs/1609.03499">Van Den Oord, et al. 2016</a>) is very similar to PixelCNN but applied to 1-D audio signals. WaveNet consists of a stack of <em>causal convolution</em> which is a convolution operation designed to respect the ordering: the prediction at a certain timestamp can only consume the data observed in the past, no dependency on the future. In PixelCNN, the causal convolution is implemented by masked convolution kernel. The causal convolution in WaveNet is simply to shift the output by a number of timestamps to the future so that the output is aligned with the last input element.</p>
<p>One big drawback of convolution layer is a very limited size of receptive field. The output can hardly depend on the input hundreds or thousands of timesteps ago, which can be a crucial requirement for modeling long sequences. WaveNet therefore adopts <em>dilated convolution</em> (<a href="https://github.com/vdumoulin/conv_arithmetic#dilated-convolution-animations">animation</a>), where the kernel is applied to an evenly-distributed subset of samples in a much larger receptive field of the input.</p>
<figure>
	<img src="wavenet.png" style="width: 100%;"  />
	<figcaption>Visualization of WaveNet models with a stack of (top) causal convolution layers and (bottom) dilated convolution layers. (Image source: <a href="https://arxiv.org/abs/1609.03499" target="_blank">Van Den Oord, et al. 2016</a>)</figcaption>
</figure>
<p>WaveNet uses the gated activation unit as the non-linear layer, as it is found to work significantly better than ReLU for modeling 1-D audio data. The residual connection is applied after the gated activation.</p>
<div>
$$
\mathbf{z} = \tanh(\mathbf{W}_{f,k}\circledast\mathbf{x})\odot\sigma(\mathbf{W}_{g,k}\circledast\mathbf{x})
$$
</div>
<p>where $\mathbf{W}_{f,k}$ and $\mathbf{W}_{g,k}$ are convolution filter and gate weight matrix of the $k$-th layer, respectively; both are learnable.</p>
<h2 id="masked-autoregressive-flow">Masked Autoregressive Flow<a hidden class="anchor" aria-hidden="true" href="#masked-autoregressive-flow">#</a></h2>
<p><strong>Masked Autoregressive Flow</strong>  (<strong>MAF</strong>; <a href="https://arxiv.org/abs/1705.07057">Papamakarios et al., 2017</a>) is a type of normalizing flows, where the transformation layer is built as an autoregressive neural network. MAF is very similar to <strong>Inverse Autoregressive Flow</strong> (IAF) introduced later. See more discussion on the relationship between MAF and IAF in the next section.</p>
<p>Given two random variables, $\mathbf{z} \sim \pi(\mathbf{z})$ and $\mathbf{x} \sim p(\mathbf{x})$ and the probability density function $\pi(\mathbf{z})$ is known, MAF aims to learn $p(\mathbf{x})$. MAF generates each $x_i$ conditioned on the past dimensions $\mathbf{x}_{1:i-1}$.</p>
<p>Precisely the conditional probability is an affine transformation of $\mathbf{z}$, where the scale and shift terms are functions of the observed part of $\mathbf{x}$.</p>
<ul>
<li>Data generation, producing a new $\mathbf{x}$:</li>
</ul>
<p>$x_i \sim p(x_i\vert\mathbf{x}_{1:i-1}) = z_i \odot \sigma_i(\mathbf{x}_{1:i-1}) + \mu_i(\mathbf{x}_{1:i-1})\text{, where }\mathbf{z} \sim \pi(\mathbf{z})$</p>
<ul>
<li>Density estimation, given a known $\mathbf{x}$:</li>
</ul>
<p>$p(\mathbf{x}) = \prod_{i=1}^D p(x_i\vert\mathbf{x}_{1:i-1})$</p>
<p>The generation procedure is sequential, so it is slow by design. While density estimation only needs one pass the network using architecture like <a href="#MADE">MADE</a>. The transformation function is trivial to inverse and the Jacobian determinant is easy to compute too.</p>
<h2 id="inverse-autoregressive-flow">Inverse Autoregressive Flow<a hidden class="anchor" aria-hidden="true" href="#inverse-autoregressive-flow">#</a></h2>
<p>Similar to MAF, <strong>Inverse autoregressive flow</strong> (<strong>IAF</strong>; <a href="https://arxiv.org/abs/1606.04934">Kingma et al., 2016</a>) models the conditional probability of the target variable as an autoregressive model too, but with a reversed flow, thus achieving a much efficient sampling process.</p>
<p>First, let&rsquo;s reverse the affine transformation in MAF:</p>
<div>
$$
z_i = \frac{x_i - \mu_i(\mathbf{x}_{1:i-1})}{\sigma_i(\mathbf{x}_{1:i-1})} = -\frac{\mu_i(\mathbf{x}_{1:i-1})}{\sigma_i(\mathbf{x}_{1:i-1})} + x_i \odot \frac{1}{\sigma_i(\mathbf{x}_{1:i-1})}
$$
</div>
<p>If let:</p>
<div>
$$
\begin{aligned}
& \tilde{\mathbf{x}} = \mathbf{z}\text{, }\tilde{p}(.) = \pi(.)\text{, }\tilde{\mathbf{x}} \sim \tilde{p}(\tilde{\mathbf{x}}) \\
& \tilde{\mathbf{z}} = \mathbf{x} \text{, }\tilde{\pi}(.) = p(.)\text{, }\tilde{\mathbf{z}} \sim \tilde{\pi}(\tilde{\mathbf{z}})\\
& \tilde{\mu}_i(\tilde{\mathbf{z}}_{1:i-1}) = \tilde{\mu}_i(\mathbf{x}_{1:i-1}) = -\frac{\mu_i(\mathbf{x}_{1:i-1})}{\sigma_i(\mathbf{x}_{1:i-1})} \\
& \tilde{\sigma}(\tilde{\mathbf{z}}_{1:i-1}) = \tilde{\sigma}(\mathbf{x}_{1:i-1}) = \frac{1}{\sigma_i(\mathbf{x}_{1:i-1})}
\end{aligned}
$$
</div>
<p>Then we would have,</p>
<div>
$$
\tilde{x}_i \sim p(\tilde{x}_i\vert\tilde{\mathbf{z}}_{1:i}) = \tilde{z}_i \odot \tilde{\sigma}_i(\tilde{\mathbf{z}}_{1:i-1}) + \tilde{\mu}_i(\tilde{\mathbf{z}}_{1:i-1})
\text{, where }\tilde{\mathbf{z}} \sim \tilde{\pi}(\tilde{\mathbf{z}})
$$
</div>
<p>IAF intends to estimate the probability density function of $\tilde{\mathbf{x}}$ given that $\tilde{\pi}(\tilde{\mathbf{z}})$ is already known. The inverse flow is an autoregressive affine transformation too, same as in MAF, but the scale and shift terms are autoregressive functions of observed variables from the known distribution $\tilde{\pi}(\tilde{\mathbf{z}})$. See the comparison between MAF and IAF in <figure>
<img src="MAF-vs-IAF.png" style="width: 100%;"  />
<figcaption>Comparison of MAF and IAF.  The variable with known density is in green while the unknown one is in red.</figcaption></p>
</figure>
<p>Computations of the individual elements $\tilde{x}_i$ do not depend on each other, so they are easily parallelizable (only one pass using MADE). The density estimation for a known $\tilde{\mathbf{x}}$ is not efficient, because we have to recover the value of $\tilde{z}_i$ in a sequential order, $\tilde{z}_i = (\tilde{x}_i - \tilde{\mu}_i(\tilde{\mathbf{z}}_{1:i-1})) / \tilde{\sigma}_i(\tilde{\mathbf{z}}_{1:i-1})$, thus D times in total.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Base distribution</th>
          <th>Target distribution</th>
          <th>Model</th>
          <th>Data generation</th>
          <th>Density estimation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>MAF</td>
          <td>$\mathbf{z}\sim\pi(\mathbf{z})$</td>
          <td>$\mathbf{x}\sim p(\mathbf{x})$</td>
          <td>$x_i = z_i \odot \sigma_i(\mathbf{x}_{1:i-1}) + \mu_i(\mathbf{x}_{1:i-1})$</td>
          <td>Sequential; slow</td>
          <td>One pass; fast</td>
      </tr>
      <tr>
          <td>IAF</td>
          <td>$\tilde{\mathbf{z}}\sim\tilde{\pi}(\tilde{\mathbf{z}})$</td>
          <td>$\tilde{\mathbf{x}}\sim\tilde{p}(\tilde{\mathbf{x}})$</td>
          <td>$\tilde{x}_i  = \tilde{z}_i \odot \tilde{\sigma}_i(\tilde{\mathbf{z}}_{1:i-1}) + \tilde{\mu}_i(\tilde{\mathbf{z}}_{1:i-1})$</td>
          <td>One pass; fast</td>
          <td>Sequential; slow</td>
      </tr>
      <tr>
          <td>&mdash;&mdash;&mdash;-</td>
          <td>&mdash;&mdash;&mdash;-</td>
          <td>&mdash;&mdash;&mdash;-</td>
          <td>&mdash;&mdash;&mdash;-</td>
          <td>&mdash;&mdash;&mdash;-</td>
          <td>&mdash;&mdash;&mdash;-</td>
      </tr>
  </tbody>
</table>
<h1 id="vae--flows">VAE + Flows<a hidden class="anchor" aria-hidden="true" href="#vae--flows">#</a></h1>
<p>In <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#vae-variational-autoencoder">Variational Autoencoder</a>, if we want to model the posterior $p(\mathbf{z}\vert\mathbf{x})$ as a more complicated distribution rather than simple Gaussian. Intuitively we can use normalizing flow to transform the base Gaussian for better density approximation. The encoder then would predict a set of scale and shift terms $(\mu_i, \sigma_i)$ which are all functions of input $\mathbf{x}$. Read the <a href="https://arxiv.org/abs/1809.05861">paper</a> for more details if interested.</p>
<hr>
<p><em>If you notice mistakes and errors in this post, don&rsquo;t hesitate to contact me at [lilian dot wengweng at gmail dot com] and I would be very happy to correct them right away!</em></p>
<p>See you in the next post :D</p>
<hr>
<p>Cited as:</p>
<pre tabindex="0"><code>@article{weng2018flow,
  title   = &#34;Flow-based Deep Generative Models&#34;,
  author  = &#34;Weng, Lilian&#34;,
  journal = &#34;lilianweng.github.io&#34;,
  year    = &#34;2018&#34;,
  url     = &#34;https://lilianweng.github.io/posts/2018-10-13-flow-models/&#34;
}
</code></pre><h1 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h1>
<p>[1] Danilo Jimenez Rezende, and Shakir Mohamed. <a href="https://arxiv.org/abs/1505.05770">&ldquo;Variational inference with normalizing flows.&rdquo;</a> ICML 2015.</p>
<p>[2] <a href="https://blog.evjang.com/2018/01/nf1.html">Normalizing Flows Tutorial, Part 1: Distributions and Determinants</a> by Eric Jang.</p>
<p>[3] <a href="https://blog.evjang.com/2018/01/nf2.html">Normalizing Flows Tutorial, Part 2: Modern Normalizing Flows</a> by Eric Jang.</p>
<p>[4] <a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html">Normalizing Flows</a> by Adam Kosiorek.</p>
<p>[5] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. <a href="https://arxiv.org/abs/1605.08803">&ldquo;Density estimation using Real NVP.&rdquo;</a> ICLR 2017.</p>
<p>[6] Laurent Dinh, David Krueger, and Yoshua Bengio. <a href="https://arxiv.org/abs/1410.8516">&ldquo;NICE: Non-linear independent components estimation.&rdquo;</a> ICLR 2015 Workshop track.</p>
<p>[7] Diederik P. Kingma, and Prafulla Dhariwal. <a href="https://arxiv.org/abs/1807.03039">&ldquo;Glow: Generative flow with invertible 1x1 convolutions.&rdquo;</a> arXiv:1807.03039 (2018).</p>
<p>[8] Germain, Mathieu, Karol Gregor, Iain Murray, and Hugo Larochelle. <a href="https://arxiv.org/abs/1502.03509">&ldquo;Made: Masked autoencoder for distribution estimation.&rdquo;</a> ICML 2015.</p>
<p>[9] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. <a href="https://arxiv.org/abs/1601.06759">&ldquo;Pixel recurrent neural networks.&rdquo;</a> ICML 2016.</p>
<p>[10] Diederik P. Kingma, et al. <a href="https://arxiv.org/abs/1606.04934">&ldquo;Improved variational inference with inverse autoregressive flow.&rdquo;</a> NIPS. 2016.</p>
<p>[11] George Papamakarios, Iain Murray, and Theo Pavlakou. <a href="https://arxiv.org/abs/1705.07057">&ldquo;Masked autoregressive flow for density estimation.&rdquo;</a> NIPS 2017.</p>
<p>[12] Jianlin Su, and Guang Wu. <a href="https://arxiv.org/abs/1809.05861">&ldquo;f-VAEs: Improve VAEs with Conditional Flows.&rdquo;</a> arXiv:1809.05861 (2018).</p>
<p>[13] Van Den Oord, Aaron, et al. <a href="https://arxiv.org/abs/1609.03499">&ldquo;WaveNet: A generative model for raw audio.&rdquo;</a> SSW. 2016.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lilianweng.github.io/tags/architecture/">Architecture</a></li>
      <li><a href="https://lilianweng.github.io/tags/generative-model/">Generative-Model</a></li>
      <li><a href="https://lilianweng.github.io/tags/image-generation/">Image-Generation</a></li>
      <li><a href="https://lilianweng.github.io/tags/math-heavy/">Math-Heavy</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lilianweng.github.io/posts/2018-11-30-meta-learning/">
    <span class="title">« </span>
    <br>
    <span>Meta-Learning: Learning to Learn Fast</span>
  </a>
  <a class="next" href="https://lilianweng.github.io/posts/2018-08-12-vae/">
    <span class="title"> »</span>
    <br>
    <span>From Autoencoder to Beta-VAE</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Flow-based Deep Generative Models on twitter"
        href="https://twitter.com/intent/tweet/?text=Flow-based%20Deep%20Generative%20Models&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-10-13-flow-models%2f&amp;hashtags=architecture%2cgenerative-model%2cimage-generation%2cmath-heavy">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Flow-based Deep Generative Models on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-10-13-flow-models%2f&amp;title=Flow-based%20Deep%20Generative%20Models&amp;summary=Flow-based%20Deep%20Generative%20Models&amp;source=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-10-13-flow-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Flow-based Deep Generative Models on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-10-13-flow-models%2f&title=Flow-based%20Deep%20Generative%20Models">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Flow-based Deep Generative Models on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-10-13-flow-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Flow-based Deep Generative Models on whatsapp"
        href="https://api.whatsapp.com/send?text=Flow-based%20Deep%20Generative%20Models%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2018-10-13-flow-models%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Flow-based Deep Generative Models on telegram"
        href="https://telegram.me/share/url?text=Flow-based%20Deep%20Generative%20Models&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-10-13-flow-models%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://lilianweng.github.io/">Lil&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
