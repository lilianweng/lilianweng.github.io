<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym | Lil&#39;Log</title>
<meta name="keywords" content="tutorial, tensorflow, reinforcement-learning" />
<meta name="description" content="The full implementation is available in lilianweng/deep-reinforcement-learning-gym
In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].
Environment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e &#34;$(curl -fsSL https://raw.">
<meta name="author" content="Lilian Weng">
<link rel="canonical" href="https://lilianweng.github.io/posts/2018-05-05-drl-implementation/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css" integrity="sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.5b9ae0304f93db6cc493f51846f012428af399c614b4f2fbdb7fa59dd4d5ef5b.js" integrity="sha256-W5rgME&#43;T22zEk/UYRvASQorzmcYUtPL723&#43;lndTV71s="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://lilianweng.github.io/favicon_peach.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lilianweng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lilianweng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lilianweng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lilianweng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-HFT45VFBX6', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym" />
<meta property="og:description" content="The full implementation is available in lilianweng/deep-reinforcement-learning-gym
In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].
Environment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e &#34;$(curl -fsSL https://raw." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lilianweng.github.io/posts/2018-05-05-drl-implementation/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-05-05T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2018-05-05T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym"/>
<meta name="twitter:description" content="The full implementation is available in lilianweng/deep-reinforcement-learning-gym
In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].
Environment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e &#34;$(curl -fsSL https://raw."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lilianweng.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Implementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym",
      "item": "https://lilianweng.github.io/posts/2018-05-05-drl-implementation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Implementing Deep Reinforcement Learning Models with Tensorflow + OpenAI Gym",
  "name": "Implementing Deep Reinforcement Learning Models with Tensorflow \u002b OpenAI Gym",
  "description": "The full implementation is available in lilianweng/deep-reinforcement-learning-gym\nIn the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].\nEnvironment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.",
  "keywords": [
    "tutorial", "tensorflow", "reinforcement-learning"
  ],
  "articleBody": " The full implementation is available in lilianweng/deep-reinforcement-learning-gym\nIn the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].\nEnvironment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" I would suggest starting a virtualenv for your development. It makes life so much easier when you have multiple projects with conflicting requirements; i.e. one works in Python 2.7 while the other is only compatible with Python 3.5+. # Install python virtualenv brew install pyenv-virtualenv # Create a virtual environment of any name you like with Python 3.6.4 support pyenv virtualenv 3.6.4 workspace # Activate the virtualenv named \"workspace\" pyenv activate workspace [*] For every new installation below, please make sure you are in the virtualenv.\nInstall OpenAI gym according to the instruction. For a minimal installation, run: git clone https://github.com/openai/gym.git cd gym pip install -e . If you are interested in playing with Atari games or other advanced packages, please continue to get a couple of system packages installed.\nbrew install cmake boost boost-python sdl2 swig wget For Atari, go to the gym directory and pip install it. This post is pretty helpful if you have troubles with ALE (arcade learning environment) installation.\npip install -e '.[atari]' Finally clone the “playground” code and install the requirements. git clone git@github.com:lilianweng/deep-reinforcement-learning-gym.git cd deep-reinforcement-learning-gym pip install -e . # install the \"playground\" project. pip install -r requirements.txt # install required packages. Gym Environment The OpenAI Gym toolkit provides a set of physical simulation environments, games, and robot simulators that we can play with and design reinforcement learning agents for. An environment object can be initialized by gym.make(\"{environment name}\":\nimport gym env = gym.make(\"MsPacman-v0\") The formats of action and observation of an environment are defined by env.action_space and env.observation_space, respectively.\nTypes of gym spaces:\ngym.spaces.Discrete(n): discrete values from 0 to n-1. gym.spaces.Box: a multi-dimensional vector of numeric values, the upper and lower bounds of each dimension are defined by Box.low and Box.high. We interact with the env through two major api calls:\nob = env.reset()\nResets the env to the original setting. Returns the initial observation. ob_next, reward, done, info = env.step(action)\nApplies one action in the env which should be compatible with env.action_space. Gets back the new observation ob_next (env.observation_space), a reward (float), a done flag (bool), and other meta information (dict). If done=True, the episode is complete and we should reset the env to restart. Read more here. Naive Q-Learning Q-learning (Watkins \u0026 Dayan, 1992) learns the action value (“Q-value”) and update it according to the Bellman equation. The key point is while estimating what is the next action, it does not follow the current policy but rather adopt the best Q value (the part in red) independently.\n$$ Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\color{red}{\\max_{a' \\in \\mathcal{A}} Q(s', a')}) $$ In a naive implementation, the Q value for all (s, a) pairs can be simply tracked in a dict. No complicated machine learning model is involved yet.\nfrom collections import defaultdict Q = defaultdict(float) gamma = 0.99 # Discounting factor alpha = 0.5 # soft update param env = gym.make(\"CartPole-v0\") actions = range(env.action_space) def update_Q(s, r, a, s_next, done): max_q_next = max([Q[s_next, a] for a in actions]) # Do not include the next state's value if currently at the terminal state. Q[s, a] += alpha * (r + gamma * max_q_next * (1.0 - done) - Q[s, a]) Most gym environments have a multi-dimensional continuous observation space (gym.spaces.Box). To make sure our Q dictionary will not explode by trying to memorize an infinite number of keys, we apply a wrapper to discretize the observation. The concept of wrappers is very powerful, with which we are capable to customize observation, action, step function, etc. of an env. No matter how many wrappers are applied, env.unwrapped always gives back the internal original environment object.\nimport gym class DiscretizedObservationWrapper(gym.ObservationWrapper): \"\"\"This wrapper converts a Box observation into a single integer. \"\"\" def __init__(self, env, n_bins=10, low=None, high=None): super().__init__(env) assert isinstance(env.observation_space, Box) low = self.observation_space.low if low is None else low high = self.observation_space.high if high is None else high self.n_bins = n_bins self.val_bins = [np.linspace(l, h, n_bins + 1) for l, h in zip(low.flatten(), high.flatten())] self.observation_space = Discrete(n_bins ** low.flatten().shape[0]) def _convert_to_one_number(self, digits): return sum([d * ((self.n_bins + 1) ** i) for i, d in enumerate(digits)]) def observation(self, observation): digits = [np.digitize([x], bins)[0] for x, bins in zip(observation.flatten(), self.val_bins)] return self._convert_to_one_number(digits) env = DiscretizedObservationWrapper( env, n_bins=8, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5] ) Let’s plug in the interaction with a gym env and update the Q function every time a new transition is generated. When picking the action, we use ε-greedy to force exploration.\nimport gym import numpy as np n_steps = 100000 epsilon = 0.1 # 10% chances to apply a random action def act(ob): if np.random.random() \u003c epsilon: # action_space.sample() is a convenient function to get a random action # that is compatible with this given action space. return env.action_space.sample() # Pick the action with highest q value. qvals = {a: q[state, a] for a in actions} max_q = max(qvals.values()) # In case multiple actions have the same maximum q value. actions_with_max_q = [a for a, q in qvals.items() if q == max_q] return np.random.choice(actions_with_max_q) ob = env.reset() rewards = [] reward = 0.0 for step in range(n_steps): a = act(ob) ob_next, r, done, _ = env.step(a) update_Q(ob, r, a, ob_next, done) reward += r if done: rewards.append(reward) reward = 0.0 ob = env.reset() else: ob = ob_next Often we start with a high epsilon and gradually decrease it during the training, known as “epsilon annealing”. The full code of QLearningPolicy is available here.\nDeep Q-Network Deep Q-network is a seminal piece of work to make the training of Q-learning more stable and more data-efficient, when the Q value is approximated with a nonlinear function. Two key ingredients are experience replay and a separately updated target network.\nThe main loss function looks like the following,\n$$ \\begin{aligned} \u0026 Y(s, a, r, s') = r + \\gamma \\max_{a'} Q_{\\theta^{-}}(s', a') \\\\ \u0026 \\mathcal{L}(\\theta) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\Big[ \\big( Y(s, a, r, s') - Q_\\theta(s, a) \\big)^2 \\Big] \\end{aligned} $$ The Q network can be a multi-layer dense neural network, a convolutional network, or a recurrent network, depending on the problem. In the full implementation of the DQN policy, it is determined by the model_type parameter, one of (“dense”, “conv”, “lstm”).\nIn the following example, I’m using a 2-layer densely connected neural network to learn Q values for the cart pole balancing problem.\nimport gym env = gym.make('CartPole-v1') # The observation space is `Box(4,)`, a 4-element vector. observation_size = env.observation_space.shape[0] We have a helper function for creating the networks below:\nimport tensorflow as tf def dense_nn(inputs, layers_sizes, scope_name): \"\"\"Creates a densely connected multi-layer neural network. inputs: the input tensor layers_sizes (list): defines the number of units in each layer. The output layer has the size layers_sizes[-1]. \"\"\" with tf.variable_scope(scope_name): for i, size in enumerate(layers_sizes): inputs = tf.layers.dense( inputs, size, # Add relu activation only for internal layers. activation=tf.nn.relu if i \u003c len(layers_sizes) - 1 else None, kernel_initializer=tf.contrib.layers.xavier_initializer(), name=scope_name + '_l' + str(i) ) return inputs The Q-network and the target network are updated with a batch of transitions (state, action, reward, state_next, done_flag). The input tensors are:\nbatch_size = 32 # A tunable hyperparameter. states = tf.placeholder(tf.float32, shape=(batch_size, observation_size), name='state') states_next = tf.placeholder(tf.float32, shape=(batch_size, observation_size), name='state_next') actions = tf.placeholder(tf.int32, shape=(batch_size,), name='action') rewards = tf.placeholder(tf.float32, shape=(batch_size,), name='reward') done_flags = tf.placeholder(tf.float32, shape=(batch_size,), name='done') We have two networks of the same structure. Both have the same network architectures with the state observation as the inputs and Q values over all the actions as the outputs.\nq = dense(states, [32, 32, 2], name='Q_primary') q_target = dense(states_next, [32, 32, 2], name='Q_target') The target network “Q_target” takes the states_next tensor as the input, because we use its prediction to select the optimal next state in the Bellman equation.\n# The prediction by the primary Q network for the actual actions. action_one_hot = tf.one_hot(actions, act_size, 1.0, 0.0, name='action_one_hot') pred = tf.reduce_sum(q * action_one_hot, reduction_indices=-1, name='q_acted') # The optimization target defined by the Bellman equation and the target network. max_q_next_by_target = tf.reduce_max(q_target, axis=-1) y = rewards + (1. - done_flags) * gamma * max_q_next_by_target # The loss measures the mean squared error between prediction and target. loss = tf.reduce_mean(tf.square(pred - tf.stop_gradient(y)), name=\"loss_mse_train\") optimizer = tf.train.AdamOptimizer(0.001).minimize(loss, name=\"adam_optim\") Note that tf.stop_gradient() on the target y, because the target network should stay fixed during the loss-minimizing gradient update.\nThe target network is updated by copying the primary Q network parameters over every C number of steps (“hard update”) or polyak averaging towards the primary network (“soft update”)\n# Get all the variables in the Q primary network. q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_primary\") # Get all the variables in the Q target network. q_target_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_target\") assert len(q_vars) == len(q_target_vars) def update_target_q_net_hard(): # Hard update sess.run([v_t.assign(v) for v_t, v in zip(q_target_vars, q_vars)]) def update_target_q_net_soft(tau=0.05): # Soft update: polyak averaging. sess.run([v_t.assign(v_t * (1. - tau) + v * tau) for v_t, v in zip(q_target_vars, q_vars)]) Double Q-Learning If we look into the standard form of the Q value target, $Y(s, a) = r + \\gamma \\max_{a’ \\in \\mathcal{A}} Q_\\theta (s’, a’)$, it is easy to notice that we use $Q_\\theta$ to select the best next action at state s’ and then apply the action value predicted by the same $Q_\\theta$. This two-step reinforcing procedure could potentially lead to overestimation of an (already) overestimated value, further leading to training instability. The solution proposed by double Q-learning (Hasselt, 2010) is to decouple the action selection and action value estimation by using two Q networks, $Q_1$ and $Q_2$: when $Q_1$ is being updated, $Q_2$ decides the best next action, and vice versa.\n$$ Y_1(s, a, r, s') = r + \\gamma Q_1 (s', \\arg\\max_{a' \\in \\mathcal{A}}Q_2(s', a'))\\\\ Y_2(s, a, r, s') = r + \\gamma Q_2 (s', \\arg\\max_{a' \\in \\mathcal{A}}Q_1(s', a')) $$ To incorporate double Q-learning into DQN, the minimum modification (Hasselt, Guez, \u0026 Silver, 2016) is to use the primary Q network to select the action while the action value is estimated by the target network:\n$$ Y(s, a, r, s') = r + \\gamma Q_{\\theta^{-}}(s', \\arg\\max_{a' \\in \\mathcal{A}} Q_\\theta(s', a')) $$ In the code, we add a new tensor for getting the action selected by the primary Q network as the input and a tensor operation for selecting this action.\nactions_next = tf.placeholder(tf.int32, shape=(None,), name='action_next') actions_selected_by_q = tf.argmax(q, axis=-1, name='action_selected') The prediction target y in the loss function becomes:\nactions_next_flatten = actions_next + tf.range(0, batch_size) * q_target.shape[1] max_q_next_target = tf.gather(tf.reshape(q_target, [-1]), actions_next_flatten) y = rewards + (1. - done_flags) * gamma * max_q_next_by_target Here I used tf.gather() to select the action values of interests.\n(Image source: tf.gather() docs) During the episode rollout, we compute the actions_next by feeding the next states’ data into the actions_selected_by_q operation.\n# batch_data is a dict with keys, ‘s', ‘a', ‘r', ‘s_next' and ‘done', containing a batch of transitions. actions_next = sess.run(actions_selected_by_q, {states: batch_data['s_next']}) Dueling Q-Network The dueling Q-network (Wang et al., 2016) is equipped with an enhanced network architecture: the output layer branches out into two heads, one for predicting state value, V, and the other for advantage, A. The Q-value is then reconstructed, $Q(s, a) = V(s) + A(s, a)$.\n$$ \\begin{aligned} A(s, a) \u0026= Q(s, a) - V(s)\\\\ V(s) \u0026= \\sum_a Q(s, a) \\pi(a \\vert s) = \\sum_a (V(s) + A(s, a)) \\pi(a \\vert s) = V(s) + \\sum_a A(s, a)\\pi(a \\vert s)\\\\ \\text{Thus, }\u0026 \\sum_a A(s, a)\\pi(a \\vert s) = 0 \\end{aligned} $$ To make sure the estimated advantage values sum up to zero, $\\sum_a A(s, a)\\pi(a \\vert s) = 0$, we deduct the mean value from the prediction.\n$$ Q(s, a) = V(s) + (A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_a A(s, a)) $$ The code change is straightforward:\nq_hidden = dense_nn(states, [32], name='Q_primary_hidden') adv = dense_nn(q_hidden, [32, env.action_space.n], name='Q_primary_adv') v = dense_nn(q_hidden, [32, 1], name='Q_primary_v') # Average dueling q = v + (adv - tf.reduce_mean(adv, reduction_indices=1, keepdims=True)) (Image source: Wang et al., 2016) Check the code for the complete flow.\nMonte-Carlo Policy Gradient I reviewed a number of popular policy gradient methods in my last post. Monte-Carlo policy gradient, also known as REINFORCE, is a classic on-policy method that learns the policy model explicitly. It uses the return estimated from a full on-policy trajectory and updates the policy parameters with policy gradient.\nThe returns are computed during rollouts and then fed into the Tensorflow graph as inputs.\n# Inputs states = tf.placeholder(tf.float32, shape=(None, obs_size), name='state') actions = tf.placeholder(tf.int32, shape=(None,), name='action') returns = tf.placeholder(tf.float32, shape=(None,), name='return') The policy network is contructed. We update the policy parameters by minimizing the loss function, $\\mathcal{L} = - (G_t - V(s)) \\log \\pi(a \\vert s)$. tf.nn.sparse_softmax_cross_entropy_with_logits() asks for the raw logits as inputs, rather then the probabilities after softmax, and that’s why we do not have a softmax layer on top of the policy network.\n# Policy network pi = dense_nn(states, [32, 32, env.action_space.n], name='pi_network') sampled_actions = tf.squeeze(tf.multinomial(pi, 1)) # For sampling actions according to probabilities. with tf.variable_scope('pi_optimize'): loss_pi = tf.reduce_mean( returns * tf.nn.sparse_softmax_cross_entropy_with_logits( logits=pi, labels=actions), name='loss_pi') optim_pi = tf.train.AdamOptimizer(0.001).minimize(loss_pi, name='adam_optim_pi') During the episode rollout, the return is calculated as follows:\n# env = gym.make(...) # gamma = 0.99 # sess = tf.Session(...) def act(ob): return sess.run(sampled_actions, {states: [ob]}) for _ in range(n_episodes): ob = env.reset() done = False obs = [] actions = [] rewards = [] returns = [] while not done: a = act(ob) new_ob, r, done, info = env.step(a) obs.append(ob) actions.append(a) rewards.append(r) ob = new_ob # Estimate returns backwards. return_so_far = 0.0 for r in rewards[::-1]: return_so_far = gamma * return_so_far + r returns.append(return_so_far) returns = returns[::-1] # Update the policy network with the data from one episode. sess.run([optim_pi], feed_dict={ states: np.array(obs), actions: np.array(actions), returns: np.array(returns), }) The full implementation of REINFORCE is here.\nActor-Critic The actor-critic algorithm learns two models at the same time, the actor for learning the best policy and the critic for estimating the state value.\nInitialize the actor network, $\\pi(a \\vert s)$ and the critic, $V(s)$ Collect a new transition (s, a, r, s’): Sample the action $a \\sim \\pi(a \\vert s)$ for the current state s, and get the reward r and the next state s'. Compute the TD target during episode rollout, $G_t = r + \\gamma V(s’)$ and TD error, $\\delta_t = r + \\gamma V(s’) - V(s)$. Update the critic network by minimizing the critic loss: $L_c = (V(s) - G_t)$. Update the actor network by minimizing the actor loss: $L_a = - \\delta_t \\log \\pi(a \\vert s)$. Set s’ = s and repeat step 2.-5. Overall the implementation looks pretty similar to REINFORCE with an extra critic network. The full implementation is here.\n# Inputs states = tf.placeholder(tf.float32, shape=(None, observation_size), name='state') actions = tf.placeholder(tf.int32, shape=(None,), name='action') td_targets = tf.placeholder(tf.float32, shape=(None,), name='td_target') # Actor: action probabilities actor = dense_nn(states, [32, 32, env.action_space.n], name='actor') # Critic: action value (Q-value) critic = dense_nn(states, [32, 32, 1], name='critic') action_ohe = tf.one_hot(actions, act_size, 1.0, 0.0, name='action_one_hot') pred_value = tf.reduce_sum(critic * action_ohe, reduction_indices=-1, name='q_acted') td_errors = td_targets - tf.reshape(pred_value, [-1]) with tf.variable_scope('critic_train'): loss_c = tf.reduce_mean(tf.square(td_errors)) optim_c = tf.train.AdamOptimizer(0.01).minimize(loss_c) with tf.variable_scope('actor_train'): loss_a = tf.reduce_mean( tf.stop_gradient(td_errors) * tf.nn.sparse_softmax_cross_entropy_with_logits( logits=actor, labels=actions), name='loss_actor') optim_a = tf.train.AdamOptimizer(0.01).minimize(loss_a) train_ops = [optim_c, optim_a] The tensorboard graph is always helpful: References [1] Tensorflow API Docs\n[2] Christopher JCH Watkins, and Peter Dayan. “Q-learning.” Machine learning 8.3-4 (1992): 279-292.\n[3] Hado Van Hasselt, Arthur Guez, and David Silver. “Deep Reinforcement Learning with Double Q-Learning.” AAAI. Vol. 16. 2016.\n[4] Hado van Hasselt. “Double Q-learning.” NIPS, 23:2613–2621, 2010.\n[5] Ziyu Wang, et al. Dueling network architectures for deep reinforcement learning. ICML. 2016.\n",
  "wordCount" : "2660",
  "inLanguage": "en",
  "datePublished": "2018-05-05T00:00:00Z",
  "dateModified": "2018-05-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lilian Weng"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://lilianweng.github.io/posts/2018-05-05-drl-implementation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lil'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://lilianweng.github.io/favicon_peach.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lilianweng.github.io/" accesskey="h" title="Lil&#39;Log (Alt + H)">Lil&#39;Log</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lilianweng.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
            <li>
                <a href="https://www.emojisearch.app/" title="emojisearch.app">
                    <span>emojisearch.app</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym
    </h1>
    <div class="post-meta">Date: May 5, 2018  |  Estimated Reading Time: 13 min  |  Author: Lilian Weng

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#environment-setup" aria-label="Environment Setup">Environment Setup</a></li>
                <li>
                    <a href="#gym-environment" aria-label="Gym Environment">Gym Environment</a></li>
                <li>
                    <a href="#naive-q-learning" aria-label="Naive Q-Learning">Naive Q-Learning</a></li>
                <li>
                    <a href="#deep-q-network" aria-label="Deep Q-Network">Deep Q-Network</a><ul>
                        
                <li>
                    <a href="#double-q-learning" aria-label="Double Q-Learning">Double Q-Learning</a></li>
                <li>
                    <a href="#dueling-q-network" aria-label="Dueling Q-Network">Dueling Q-Network</a></li></ul>
                </li>
                <li>
                    <a href="#monte-carlo-policy-gradient" aria-label="Monte-Carlo Policy Gradient">Monte-Carlo Policy Gradient</a></li>
                <li>
                    <a href="#actor-critic" aria-label="Actor-Critic">Actor-Critic</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><!-- Let's see how to implement a number of classic deep reinforcement learning models in code. -->
<p>The full implementation is available in <a href="https://github.com/lilianweng/deep-reinforcement-learning-gym">lilianweng/deep-reinforcement-learning-gym</a></p>
<p>In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI <a href="https://github.com/openai/gym">gym</a> environment. The full version of the code in this tutorial is available in <a href="https://github.com/lilianweng/deep-reinforcement-learning-gym">[lilian/deep-reinforcement-learning-gym]</a>.</p>
<h1 id="environment-setup">Environment Setup<a hidden class="anchor" aria-hidden="true" href="#environment-setup">#</a></h1>
<ol start="0">
<li>Make sure you have <a href="https://docs.brew.sh/Installation">Homebrew</a> installed:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>/usr/bin/ruby -e <span style="color:#e6db74">&#34;</span><span style="color:#66d9ef">$(</span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install<span style="color:#66d9ef">)</span><span style="color:#e6db74">&#34;</span>
</span></span></code></pre></div><ol>
<li>I would suggest starting a virtualenv for your development. It makes life so much easier when you have multiple projects with conflicting requirements; i.e. one works in Python 2.7 while the other is only compatible with Python 3.5+.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Install python virtualenv</span>
</span></span><span style="display:flex;"><span>brew install pyenv-virtualenv
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a virtual environment of any name you like with Python 3.6.4 support</span>
</span></span><span style="display:flex;"><span>pyenv virtualenv 3.6.4 workspace
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Activate the virtualenv named &#34;workspace&#34;</span>
</span></span><span style="display:flex;"><span>pyenv activate workspace
</span></span></code></pre></div><p><em>[*] For every new installation below, please make sure you are in the virtualenv.</em></p>
<ol start="2">
<li>Install OpenAI gym according to the <a href="https://github.com/openai/gym#installation">instruction</a>. For a minimal installation, run:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/openai/gym.git 
</span></span><span style="display:flex;"><span>cd gym 
</span></span><span style="display:flex;"><span>pip install -e .
</span></span></code></pre></div><p>If you are interested in playing with Atari games or other advanced packages, please continue to get a couple of system packages installed.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>brew install cmake boost boost-python sdl2 swig wget
</span></span></code></pre></div><p>For Atari, go to the gym directory and pip install it. This <a href="http://alvinwan.com/installing-arcade-learning-environment-with-python3-on-macosx/">post</a> is pretty helpful if you have troubles with ALE (arcade learning environment) installation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install -e <span style="color:#e6db74">&#39;.[atari]&#39;</span>
</span></span></code></pre></div><ol start="3">
<li>Finally clone the &ldquo;playground&rdquo; code and install the requirements.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone git@github.com:lilianweng/deep-reinforcement-learning-gym.git
</span></span><span style="display:flex;"><span>cd deep-reinforcement-learning-gym
</span></span><span style="display:flex;"><span>pip install -e .  <span style="color:#75715e"># install the &#34;playground&#34; project.</span>
</span></span><span style="display:flex;"><span>pip install -r requirements.txt  <span style="color:#75715e"># install required packages.</span>
</span></span></code></pre></div><h1 id="gym-environment">Gym Environment<a hidden class="anchor" aria-hidden="true" href="#gym-environment">#</a></h1>
<p>The <a href="https://gym.openai.com/">OpenAI Gym</a> toolkit provides a set of physical simulation environments, games, and robot simulators that we can play with and design reinforcement learning agents for. An environment object can be initialized by <code>gym.make(&quot;{environment name}&quot;</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gym
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#34;MsPacman-v0&#34;</span>)
</span></span></code></pre></div><img src="pacman-original.gif" class="center" />
<p>The formats of action and observation of an environment are defined by <code>env.action_space</code> and <code>env.observation_space</code>, respectively.</p>
<p>Types of gym <a href="https://gym.openai.com/docs/#spaces">spaces</a>:</p>
<ul>
<li><code>gym.spaces.Discrete(n)</code>: discrete values from 0 to n-1.</li>
<li><code>gym.spaces.Box</code>: a multi-dimensional vector of numeric values, the upper and lower bounds of each dimension are defined by <code>Box.low</code> and <code>Box.high</code>.</li>
</ul>
<p>We interact with the env through two major api calls:</p>
<p><strong><code>ob = env.reset()</code></strong></p>
<ul>
<li>Resets the env to the original setting.</li>
<li>Returns the initial observation.</li>
</ul>
<p><strong><code>ob_next, reward, done, info = env.step(action)</code></strong></p>
<ul>
<li>Applies one action in the env which should be compatible with <code>env.action_space</code>.</li>
<li>Gets back the new observation <code>ob_next</code> (env.observation_space), a reward (float), a <code>done</code> flag (bool), and other meta information (dict). If <code>done=True</code>, the episode is complete and we should reset the env to restart. Read more <a href="https://gym.openai.com/docs/#observations">here</a>.</li>
</ul>
<h1 id="naive-q-learning">Naive Q-Learning<a hidden class="anchor" aria-hidden="true" href="#naive-q-learning">#</a></h1>
<p><a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control">Q-learning</a> (Watkins &amp; Dayan, 1992) learns the action value (&ldquo;Q-value&rdquo;) and update it according to the <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#bellman-equations">Bellman equation</a>. The key point is while estimating what is the next action, it does not follow the current policy but rather adopt the best Q value (the part in red) independently.</p>
<div>
$$
Q(s, a) \leftarrow (1 - \alpha) Q(s, a) + \alpha (r + \gamma \color{red}{\max_{a' \in \mathcal{A}} Q(s', a')})
$$
</div>
<p>In a naive implementation, the Q value for all (s, a) pairs can be simply tracked in a dict. No complicated machine learning model is involved yet.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> defaultdict
</span></span><span style="display:flex;"><span>Q <span style="color:#f92672">=</span> defaultdict(float)
</span></span><span style="display:flex;"><span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.99</span>  <span style="color:#75715e"># Discounting factor</span>
</span></span><span style="display:flex;"><span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># soft update param</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#34;CartPole-v0&#34;</span>)
</span></span><span style="display:flex;"><span>actions <span style="color:#f92672">=</span> range(env<span style="color:#f92672">.</span>action_space)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_Q</span>(s, r, a, s_next, done):
</span></span><span style="display:flex;"><span>    max_q_next <span style="color:#f92672">=</span> max([Q[s_next, a] <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> actions]) 
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Do not include the next state&#39;s value if currently at the terminal state.</span>
</span></span><span style="display:flex;"><span>    Q[s, a] <span style="color:#f92672">+=</span> alpha <span style="color:#f92672">*</span> (r <span style="color:#f92672">+</span> gamma <span style="color:#f92672">*</span> max_q_next <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> done) <span style="color:#f92672">-</span> Q[s, a])
</span></span></code></pre></div><p>Most gym environments have a multi-dimensional continuous observation space (<code>gym.spaces.Box</code>). To make sure our Q dictionary will not explode by trying to memorize an infinite number of keys, we apply a wrapper to discretize the observation. The concept of <a href="https://github.com/openai/gym/tree/master/gym/wrappers">wrappers</a> is very powerful, with which we are capable to customize observation, action, step function, etc. of an env. No matter how many wrappers are applied, <code>env.unwrapped</code> always gives back the internal original environment object.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gym
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DiscretizedObservationWrapper</span>(gym<span style="color:#f92672">.</span>ObservationWrapper):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;This wrapper converts a Box observation into a single integer.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, env, n_bins<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, low<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, high<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__(env)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> isinstance(env<span style="color:#f92672">.</span>observation_space, Box)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        low <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>low <span style="color:#66d9ef">if</span> low <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> low
</span></span><span style="display:flex;"><span>        high <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>high <span style="color:#66d9ef">if</span> high <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> high
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_bins <span style="color:#f92672">=</span> n_bins
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>val_bins <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>linspace(l, h, n_bins <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">for</span> l, h <span style="color:#f92672">in</span>
</span></span><span style="display:flex;"><span>                         zip(low<span style="color:#f92672">.</span>flatten(), high<span style="color:#f92672">.</span>flatten())]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>observation_space <span style="color:#f92672">=</span> Discrete(n_bins <span style="color:#f92672">**</span> low<span style="color:#f92672">.</span>flatten()<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_convert_to_one_number</span>(self, digits):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> sum([d <span style="color:#f92672">*</span> ((self<span style="color:#f92672">.</span>n_bins <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">**</span> i) <span style="color:#66d9ef">for</span> i, d <span style="color:#f92672">in</span> enumerate(digits)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">observation</span>(self, observation):
</span></span><span style="display:flex;"><span>        digits <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>digitize([x], bins)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>                  <span style="color:#66d9ef">for</span> x, bins <span style="color:#f92672">in</span> zip(observation<span style="color:#f92672">.</span>flatten(), self<span style="color:#f92672">.</span>val_bins)]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>_convert_to_one_number(digits)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> DiscretizedObservationWrapper(
</span></span><span style="display:flex;"><span>    env, 
</span></span><span style="display:flex;"><span>    n_bins<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, 
</span></span><span style="display:flex;"><span>    low<span style="color:#f92672">=</span>[<span style="color:#f92672">-</span><span style="color:#ae81ff">2.4</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.42</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">3.5</span>], 
</span></span><span style="display:flex;"><span>    high<span style="color:#f92672">=</span>[<span style="color:#ae81ff">2.4</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">0.42</span>, <span style="color:#ae81ff">3.5</span>]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Let&rsquo;s plug in the interaction with a gym env and update the Q function every time a new transition is generated. When picking the action, we use ε-greedy to force exploration.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gym
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>n_steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">100000</span>
</span></span><span style="display:flex;"><span>epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>  <span style="color:#75715e"># 10% chances to apply a random action</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">act</span>(ob):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>random() <span style="color:#f92672">&lt;</span> epsilon:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># action_space.sample() is a convenient function to get a random action</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># that is compatible with this given action space.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Pick the action with highest q value.</span>
</span></span><span style="display:flex;"><span>    qvals <span style="color:#f92672">=</span> {a: q[state, a] <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> actions}
</span></span><span style="display:flex;"><span>    max_q <span style="color:#f92672">=</span> max(qvals<span style="color:#f92672">.</span>values())
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># In case multiple actions have the same maximum q value.</span>
</span></span><span style="display:flex;"><span>    actions_with_max_q <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, q <span style="color:#f92672">in</span> qvals<span style="color:#f92672">.</span>items() <span style="color:#66d9ef">if</span> q <span style="color:#f92672">==</span> max_q]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(actions_with_max_q)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ob <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>rewards <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(n_steps):
</span></span><span style="display:flex;"><span>    a <span style="color:#f92672">=</span> act(ob)
</span></span><span style="display:flex;"><span>    ob_next, r, done, _ <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(a)
</span></span><span style="display:flex;"><span>    update_Q(ob, r, a, ob_next, done)
</span></span><span style="display:flex;"><span>    reward <span style="color:#f92672">+=</span> r
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> done:
</span></span><span style="display:flex;"><span>        rewards<span style="color:#f92672">.</span>append(reward)
</span></span><span style="display:flex;"><span>        reward <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>        ob <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        ob <span style="color:#f92672">=</span> ob_next
</span></span></code></pre></div><p>Often we start with a high <code>epsilon</code> and gradually decrease it during the training, known as &ldquo;epsilon annealing&rdquo;. The full code of <code>QLearningPolicy</code> is available <a href="https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/qlearning.py">here</a>.</p>
<h1 id="deep-q-network">Deep Q-Network<a hidden class="anchor" aria-hidden="true" href="#deep-q-network">#</a></h1>
<p><a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#deep-q-network">Deep Q-network</a> is a seminal piece of work to make the training of Q-learning more stable and more data-efficient, when the Q value is approximated with a nonlinear function. Two key ingredients are experience replay and a separately updated target network.</p>
<p>The main loss function looks like the following,</p>
<div>
$$
\begin{aligned}
& Y(s, a, r, s') = r + \gamma \max_{a'} Q_{\theta^{-}}(s', a') \\
& \mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Big[ \big( Y(s, a, r, s') - Q_\theta(s, a) \big)^2 \Big]
\end{aligned}
$$
</div>
<p>The Q network can be a multi-layer dense neural network, a convolutional network, or a recurrent network, depending on the problem. In the <a href="https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/dqn.py">full implementation</a> of the DQN policy, it is determined by the <code>model_type</code> parameter, one of (&ldquo;dense&rdquo;, &ldquo;conv&rdquo;, &ldquo;lstm&rdquo;).</p>
<p>In the following example,  I&rsquo;m using a 2-layer densely connected neural network to learn Q values for the cart pole balancing problem.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gym
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;CartPole-v1&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The observation space is `Box(4,)`, a 4-element vector.</span>
</span></span><span style="display:flex;"><span>observation_size <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><p>We have a helper function for creating the networks below:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dense_nn</span>(inputs, layers_sizes, scope_name):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Creates a densely connected multi-layer neural network.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    inputs: the input tensor
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    layers_sizes (list&lt;int&gt;): defines the number of units in each layer. The output 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        layer has the size layers_sizes[-1].
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>variable_scope(scope_name):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i, size <span style="color:#f92672">in</span> enumerate(layers_sizes):
</span></span><span style="display:flex;"><span>            inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>dense(
</span></span><span style="display:flex;"><span>                inputs,
</span></span><span style="display:flex;"><span>                size,
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># Add relu activation only for internal layers.</span>
</span></span><span style="display:flex;"><span>                activation<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>relu <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&lt;</span> len(layers_sizes) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                kernel_initializer<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>contrib<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>xavier_initializer(),
</span></span><span style="display:flex;"><span>                name<span style="color:#f92672">=</span>scope_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;_l&#39;</span> <span style="color:#f92672">+</span> str(i)
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> inputs
</span></span></code></pre></div><p>The Q-network and the target network are updated with a batch of transitions (state, action, reward, state_next, done_flag). The input tensors are:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>  <span style="color:#75715e"># A tunable hyperparameter.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>states <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(batch_size, observation_size), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;state&#39;</span>)
</span></span><span style="display:flex;"><span>states_next <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(batch_size, observation_size), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;state_next&#39;</span>)
</span></span><span style="display:flex;"><span>actions <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>int32, shape<span style="color:#f92672">=</span>(batch_size,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;action&#39;</span>)
</span></span><span style="display:flex;"><span>rewards <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(batch_size,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;reward&#39;</span>)
</span></span><span style="display:flex;"><span>done_flags <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(batch_size,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;done&#39;</span>)
</span></span></code></pre></div><p>We have two networks of the same structure. Both have the same network architectures with the state observation as the inputs and Q values over all the actions as the outputs.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>q <span style="color:#f92672">=</span> dense(states, [<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">2</span>], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Q_primary&#39;</span>)
</span></span><span style="display:flex;"><span>q_target <span style="color:#f92672">=</span> dense(states_next, [<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">2</span>], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Q_target&#39;</span>)
</span></span></code></pre></div><p>The target network &ldquo;Q_target&rdquo; takes the <code>states_next</code> tensor as the input, because we use its prediction to select the optimal next state in the Bellman equation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># The prediction by the primary Q network for the actual actions.</span>
</span></span><span style="display:flex;"><span>action_one_hot <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>one_hot(actions, act_size, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;action_one_hot&#39;</span>)
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_sum(q <span style="color:#f92672">*</span> action_one_hot, reduction_indices<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;q_acted&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The optimization target defined by the Bellman equation and the target network.</span>
</span></span><span style="display:flex;"><span>max_q_next_by_target <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_max(q_target, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> rewards <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> done_flags) <span style="color:#f92672">*</span> gamma <span style="color:#f92672">*</span> max_q_next_by_target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The loss measures the mean squared error between prediction and target.</span>
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(pred <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>stop_gradient(y)), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;loss_mse_train&#34;</span>)
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(<span style="color:#ae81ff">0.001</span>)<span style="color:#f92672">.</span>minimize(loss, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;adam_optim&#34;</span>)
</span></span></code></pre></div><p>Note that <a href="https://www.tensorflow.org/api_docs/python/tf/stop_gradient">tf.stop_gradient()</a> on the target y, because the target network should stay fixed during the loss-minimizing gradient update.</p>
<img src="dqn-tensorboard-graph.png" class="center" />
<p>The target network is updated by copying the primary Q network parameters over every <code>C</code> number of steps (&ldquo;hard update&rdquo;) or polyak averaging towards the primary network (&ldquo;soft update&rdquo;)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Get all the variables in the Q primary network.</span>
</span></span><span style="display:flex;"><span>q_vars <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_collection(tf<span style="color:#f92672">.</span>GraphKeys<span style="color:#f92672">.</span>GLOBAL_VARIABLES, scope<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Q_primary&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get all the variables in the Q target network.</span>
</span></span><span style="display:flex;"><span>q_target_vars <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_collection(tf<span style="color:#f92672">.</span>GraphKeys<span style="color:#f92672">.</span>GLOBAL_VARIABLES, scope<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Q_target&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">assert</span> len(q_vars) <span style="color:#f92672">==</span> len(q_target_vars)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_target_q_net_hard</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Hard update</span>
</span></span><span style="display:flex;"><span>    sess<span style="color:#f92672">.</span>run([v_t<span style="color:#f92672">.</span>assign(v) <span style="color:#66d9ef">for</span> v_t, v <span style="color:#f92672">in</span> zip(q_target_vars, q_vars)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_target_q_net_soft</span>(tau<span style="color:#f92672">=</span><span style="color:#ae81ff">0.05</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Soft update: polyak averaging.</span>
</span></span><span style="display:flex;"><span>    sess<span style="color:#f92672">.</span>run([v_t<span style="color:#f92672">.</span>assign(v_t <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> tau) <span style="color:#f92672">+</span> v <span style="color:#f92672">*</span> tau) <span style="color:#66d9ef">for</span> v_t, v <span style="color:#f92672">in</span> zip(q_target_vars, q_vars)])
</span></span></code></pre></div><h2 id="double-q-learning">Double Q-Learning<a hidden class="anchor" aria-hidden="true" href="#double-q-learning">#</a></h2>
<p>If we look into the standard form of the Q value target, $Y(s, a) = r + \gamma \max_{a&rsquo; \in \mathcal{A}} Q_\theta (s&rsquo;, a&rsquo;)$, it is easy to notice that we use $Q_\theta$ to select the best next action at state s&rsquo; and then apply the action value predicted by the same $Q_\theta$. This two-step reinforcing procedure could potentially lead to overestimation of an (already) overestimated value, further leading to training instability. The solution proposed by double Q-learning (<a href="http://papers.nips.cc/paper/3964-double-q-learning.pdf">Hasselt, 2010</a>) is to decouple the action selection and action value estimation by using two Q networks, $Q_1$ and $Q_2$: when $Q_1$ is being updated, $Q_2$ decides the best next action, and vice versa.</p>
<div>
$$
Y_1(s, a, r, s') = r + \gamma Q_1 (s', \arg\max_{a' \in \mathcal{A}}Q_2(s', a'))\\
Y_2(s, a, r, s') = r + \gamma Q_2 (s', \arg\max_{a' \in \mathcal{A}}Q_1(s', a'))
$$
</div>
<p>To incorporate double Q-learning into DQN, the minimum modification (<a href="https://arxiv.org/pdf/1509.06461.pdf">Hasselt, Guez, &amp; Silver, 2016</a>) is to use the primary Q network to select the action while the action value is estimated by the target network:</p>
<div>
$$
Y(s, a, r, s') = r + \gamma Q_{\theta^{-}}(s', \arg\max_{a' \in \mathcal{A}} Q_\theta(s', a'))
$$
</div>
<p>In the code, we add a new tensor for getting the action selected by the primary Q network as the input and a tensor operation for selecting this action.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>actions_next <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>int32, shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;action_next&#39;</span>)
</span></span><span style="display:flex;"><span>actions_selected_by_q <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>argmax(q, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;action_selected&#39;</span>)
</span></span></code></pre></div><p>The prediction target y in the loss function becomes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>actions_next_flatten <span style="color:#f92672">=</span> actions_next <span style="color:#f92672">+</span> tf<span style="color:#f92672">.</span>range(<span style="color:#ae81ff">0</span>, batch_size) <span style="color:#f92672">*</span> q_target<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>max_q_next_target <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>gather(tf<span style="color:#f92672">.</span>reshape(q_target, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]), actions_next_flatten)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> rewards <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> done_flags) <span style="color:#f92672">*</span> gamma <span style="color:#f92672">*</span> max_q_next_by_target
</span></span></code></pre></div><p>Here I used <a href="https://www.tensorflow.org/api_docs/python/tf/gather">tf.gather()</a> to select the action values of interests.</p>
<img src="tf_gather.png" style="width: 60%;" class="center" />
<figcaption>(Image source: <a ref="https://www.tensorflow.org/api_docs/python/tf/gather" target="_blank">tf.gather() docs</a>)</figcaption>
<p>During the episode rollout, we compute the <code>actions_next</code> by feeding the next states&rsquo; data into the <code>actions_selected_by_q</code> operation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># batch_data is a dict with keys, ‘s&#39;, ‘a&#39;, ‘r&#39;, ‘s_next&#39; and ‘done&#39;, containing a batch of transitions.</span>
</span></span><span style="display:flex;"><span>actions_next <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run(actions_selected_by_q, {states: batch_data[<span style="color:#e6db74">&#39;s_next&#39;</span>]})
</span></span></code></pre></div><h2 id="dueling-q-network">Dueling Q-Network<a hidden class="anchor" aria-hidden="true" href="#dueling-q-network">#</a></h2>
<p>The dueling Q-network (<a href="https://arxiv.org/pdf/1511.06581.pdf">Wang et al., 2016</a>) is equipped with an enhanced network architecture: the output layer branches out into two heads, one for predicting state value, V, and the other for <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#value-function">advantage</a>, A. The Q-value is then reconstructed, $Q(s, a) = V(s) + A(s, a)$.</p>
<div>
$$
\begin{aligned}
A(s, a) &= Q(s, a) - V(s)\\
V(s) &= \sum_a Q(s, a) \pi(a \vert s) = \sum_a (V(s) + A(s, a)) \pi(a \vert s) = V(s) + \sum_a A(s, a)\pi(a \vert s)\\
\text{Thus, }& \sum_a A(s, a)\pi(a \vert s) = 0
\end{aligned}
$$
</div>
<p>To make sure the estimated advantage values sum up to zero, $\sum_a A(s, a)\pi(a \vert s) = 0$, we deduct the mean value from the prediction.</p>
<div>
$$
Q(s, a) = V(s) + (A(s, a) - \frac{1}{|\mathcal{A}|} \sum_a A(s, a))
$$
</div>
<p>The code change is straightforward:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>q_hidden <span style="color:#f92672">=</span> dense_nn(states, [<span style="color:#ae81ff">32</span>], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Q_primary_hidden&#39;</span>)
</span></span><span style="display:flex;"><span>adv <span style="color:#f92672">=</span> dense_nn(q_hidden, [<span style="color:#ae81ff">32</span>, env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Q_primary_adv&#39;</span>)
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> dense_nn(q_hidden, [<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">1</span>], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Q_primary_v&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Average dueling</span>
</span></span><span style="display:flex;"><span>q <span style="color:#f92672">=</span> v <span style="color:#f92672">+</span> (adv <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>reduce_mean(adv, reduction_indices<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span></code></pre></div><img src="dueling-q-network.png" class="center" />
<figcaption>(Image source: <a href="https://arxiv.org/pdf/1511.06581.pdf" target="_blank">Wang et al., 2016</a>)</figcaption>
<p>Check the <a href="https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/dqn.py">code</a> for the complete flow.</p>
<h1 id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient<a hidden class="anchor" aria-hidden="true" href="#monte-carlo-policy-gradient">#</a></h1>
<p>I reviewed a number of popular policy gradient methods in my <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">last post</a>. Monte-Carlo policy gradient, also known as <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#reinforce">REINFORCE</a>, is a classic on-policy method that learns the policy model explicitly. It uses the return estimated from a full on-policy trajectory and updates the policy parameters with policy gradient.</p>
<p>The returns are computed during rollouts and then fed into the Tensorflow graph as inputs.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Inputs</span>
</span></span><span style="display:flex;"><span>states <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>, obs_size), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;state&#39;</span>)
</span></span><span style="display:flex;"><span>actions <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>int32, shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;action&#39;</span>)
</span></span><span style="display:flex;"><span>returns <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;return&#39;</span>)
</span></span></code></pre></div><p>The policy network is contructed. We update the policy parameters by minimizing the loss function, $\mathcal{L} = - (G_t - V(s)) \log \pi(a \vert s)$.
<a href="https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits">tf.nn.sparse_softmax_cross_entropy_with_logits()</a> asks for the raw logits as inputs, rather then the probabilities after softmax, and that&rsquo;s why we do not have a softmax layer on top of the policy network.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Policy network</span>
</span></span><span style="display:flex;"><span>pi <span style="color:#f92672">=</span> dense_nn(states, [<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;pi_network&#39;</span>)
</span></span><span style="display:flex;"><span>sampled_actions <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>squeeze(tf<span style="color:#f92672">.</span>multinomial(pi, <span style="color:#ae81ff">1</span>))  <span style="color:#75715e"># For sampling actions according to probabilities.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>variable_scope(<span style="color:#e6db74">&#39;pi_optimize&#39;</span>):
</span></span><span style="display:flex;"><span>    loss_pi <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(
</span></span><span style="display:flex;"><span>        returns <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>sparse_softmax_cross_entropy_with_logits(
</span></span><span style="display:flex;"><span>            logits<span style="color:#f92672">=</span>pi, labels<span style="color:#f92672">=</span>actions), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;loss_pi&#39;</span>)
</span></span><span style="display:flex;"><span>    optim_pi <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(<span style="color:#ae81ff">0.001</span>)<span style="color:#f92672">.</span>minimize(loss_pi, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam_optim_pi&#39;</span>)
</span></span></code></pre></div><p>During the episode rollout, the return is calculated as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># env = gym.make(...)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># gamma = 0.99</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># sess = tf.Session(...)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">act</span>(ob):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> sess<span style="color:#f92672">.</span>run(sampled_actions, {states: [ob]})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(n_episodes):
</span></span><span style="display:flex;"><span>    ob <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    obs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    actions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    rewards <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    returns <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        a <span style="color:#f92672">=</span> act(ob)
</span></span><span style="display:flex;"><span>        new_ob, r, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(a)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        obs<span style="color:#f92672">.</span>append(ob)
</span></span><span style="display:flex;"><span>        actions<span style="color:#f92672">.</span>append(a)
</span></span><span style="display:flex;"><span>        rewards<span style="color:#f92672">.</span>append(r)
</span></span><span style="display:flex;"><span>        ob <span style="color:#f92672">=</span> new_ob
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Estimate returns backwards.</span>
</span></span><span style="display:flex;"><span>    return_so_far <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> rewards[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]:
</span></span><span style="display:flex;"><span>        return_so_far <span style="color:#f92672">=</span> gamma <span style="color:#f92672">*</span> return_so_far <span style="color:#f92672">+</span> r
</span></span><span style="display:flex;"><span>        returns<span style="color:#f92672">.</span>append(return_so_far)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    returns <span style="color:#f92672">=</span> returns[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Update the policy network with the data from one episode.</span>
</span></span><span style="display:flex;"><span>    sess<span style="color:#f92672">.</span>run([optim_pi], feed_dict<span style="color:#f92672">=</span>{
</span></span><span style="display:flex;"><span>        states: np<span style="color:#f92672">.</span>array(obs),
</span></span><span style="display:flex;"><span>        actions: np<span style="color:#f92672">.</span>array(actions),
</span></span><span style="display:flex;"><span>        returns: np<span style="color:#f92672">.</span>array(returns),
</span></span><span style="display:flex;"><span>    })
</span></span></code></pre></div><p>The full implementation of REINFORCE is <a href="https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/reinforce.py">here</a>.</p>
<h1 id="actor-critic">Actor-Critic<a hidden class="anchor" aria-hidden="true" href="#actor-critic">#</a></h1>
<p>The <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#actor-critic">actor-critic</a> algorithm learns two models at the same time, the actor for learning the best policy and the critic for estimating the state value.</p>
<ol>
<li>Initialize the actor network, $\pi(a \vert s)$ and the critic, $V(s)$</li>
<li>Collect a new transition (s, a, r, s&rsquo;): Sample the action $a \sim \pi(a \vert s)$ for the current state s, and get the reward r and the next state s'.</li>
<li>Compute the TD target during episode rollout, $G_t = r + \gamma V(s&rsquo;)$ and TD error, $\delta_t = r + \gamma V(s&rsquo;) - V(s)$.</li>
<li>Update the critic network by minimizing the critic loss: $L_c = (V(s) - G_t)$.</li>
<li>Update the actor network by minimizing the actor loss: $L_a = - \delta_t \log \pi(a \vert s)$.</li>
<li>Set s&rsquo; = s and repeat step 2.-5.</li>
</ol>
<p>Overall the implementation looks pretty similar to REINFORCE with an extra critic network. The full implementation is here.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Inputs</span>
</span></span><span style="display:flex;"><span>states <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>, observation_size), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;state&#39;</span>)
</span></span><span style="display:flex;"><span>actions <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>int32, shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;action&#39;</span>)
</span></span><span style="display:flex;"><span>td_targets <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>,), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;td_target&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Actor: action probabilities</span>
</span></span><span style="display:flex;"><span>actor <span style="color:#f92672">=</span> dense_nn(states, [<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;actor&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Critic: action value (Q-value)</span>
</span></span><span style="display:flex;"><span>critic <span style="color:#f92672">=</span> dense_nn(states, [<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">1</span>], name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;critic&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>action_ohe <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>one_hot(actions, act_size, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;action_one_hot&#39;</span>)
</span></span><span style="display:flex;"><span>pred_value <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_sum(critic <span style="color:#f92672">*</span> action_ohe, reduction_indices<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;q_acted&#39;</span>)
</span></span><span style="display:flex;"><span>td_errors <span style="color:#f92672">=</span> td_targets <span style="color:#f92672">-</span> tf<span style="color:#f92672">.</span>reshape(pred_value, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>variable_scope(<span style="color:#e6db74">&#39;critic_train&#39;</span>):
</span></span><span style="display:flex;"><span>    loss_c <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>square(td_errors))
</span></span><span style="display:flex;"><span>    optim_c <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(<span style="color:#ae81ff">0.01</span>)<span style="color:#f92672">.</span>minimize(loss_c)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>variable_scope(<span style="color:#e6db74">&#39;actor_train&#39;</span>):
</span></span><span style="display:flex;"><span>    loss_a <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(
</span></span><span style="display:flex;"><span>        tf<span style="color:#f92672">.</span>stop_gradient(td_errors) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>sparse_softmax_cross_entropy_with_logits(
</span></span><span style="display:flex;"><span>            logits<span style="color:#f92672">=</span>actor, labels<span style="color:#f92672">=</span>actions),
</span></span><span style="display:flex;"><span>        name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;loss_actor&#39;</span>)
</span></span><span style="display:flex;"><span>    optim_a <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(<span style="color:#ae81ff">0.01</span>)<span style="color:#f92672">.</span>minimize(loss_a)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_ops <span style="color:#f92672">=</span> [optim_c, optim_a]
</span></span></code></pre></div><p>The tensorboard graph is always helpful:
<img src="actor-critic-tensorboard-graph.png" class="center" /></p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<p>[1] <a href="https://www.tensorflow.org/api_docs/">Tensorflow API Docs</a></p>
<p>[2] Christopher JCH Watkins, and Peter Dayan. <a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf">&ldquo;Q-learning.&rdquo;</a> Machine learning 8.3-4 (1992): 279-292.</p>
<p>[3] Hado Van Hasselt, Arthur Guez, and David Silver. <a href="https://arxiv.org/pdf/1509.06461.pdf">&ldquo;Deep Reinforcement Learning with Double Q-Learning.&rdquo;</a> AAAI. Vol. 16. 2016.</p>
<p>[4] Hado van Hasselt. <a href="http://papers.nips.cc/paper/3964-double-q-learning.pdf">&ldquo;Double Q-learning.&rdquo;</a> NIPS, 23:2613–2621, 2010.</p>
<p>[5] Ziyu Wang, et al. <a href="https://arxiv.org/pdf/1511.06581.pdf">Dueling network architectures for deep reinforcement learning.</a> ICML. 2016.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://lilianweng.github.io/tags/tutorial/">tutorial</a></li>
      <li><a href="https://lilianweng.github.io/tags/tensorflow/">tensorflow</a></li>
      <li><a href="https://lilianweng.github.io/tags/reinforcement-learning/">reinforcement-learning</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://lilianweng.github.io/posts/2018-06-24-attention/">
    <span class="title">« </span>
    <br>
    <span>Attention? Attention!</span>
  </a>
  <a class="next" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">
    <span class="title"> »</span>
    <br>
    <span>Policy Gradient Algorithms</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym on twitter"
        href="https://twitter.com/intent/tweet/?text=Implementing%20Deep%20Reinforcement%20Learning%20Models%20with%20Tensorflow%20%2b%20OpenAI%20Gym&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-05-05-drl-implementation%2f&amp;hashtags=tutorial%2ctensorflow%2creinforcement-learning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-05-05-drl-implementation%2f&amp;title=Implementing%20Deep%20Reinforcement%20Learning%20Models%20with%20Tensorflow%20%2b%20OpenAI%20Gym&amp;summary=Implementing%20Deep%20Reinforcement%20Learning%20Models%20with%20Tensorflow%20%2b%20OpenAI%20Gym&amp;source=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-05-05-drl-implementation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-05-05-drl-implementation%2f&title=Implementing%20Deep%20Reinforcement%20Learning%20Models%20with%20Tensorflow%20%2b%20OpenAI%20Gym">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-05-05-drl-implementation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym on whatsapp"
        href="https://api.whatsapp.com/send?text=Implementing%20Deep%20Reinforcement%20Learning%20Models%20with%20Tensorflow%20%2b%20OpenAI%20Gym%20-%20https%3a%2f%2flilianweng.github.io%2fposts%2f2018-05-05-drl-implementation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym on telegram"
        href="https://telegram.me/share/url?text=Implementing%20Deep%20Reinforcement%20Learning%20Models%20with%20Tensorflow%20%2b%20OpenAI%20Gym&amp;url=https%3a%2f%2flilianweng.github.io%2fposts%2f2018-05-05-drl-implementation%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://lilianweng.github.io/">Lil&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
