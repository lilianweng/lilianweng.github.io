<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Posts | Lil&#39;Log</title>
<meta name="keywords" content="" />
<meta name="description" content="Posts - Lil&#39;Log">
<meta name="author" content="Lilian Weng">
<link rel="canonical" href="https://lilianweng.github.io/posts/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css" integrity="sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lilianweng.github.io/favicon_peach.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lilianweng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lilianweng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lilianweng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lilianweng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://lilianweng.github.io/posts/index.xml">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-HFT45VFBX6', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Posts" />
<meta property="og:description" content="Document my learning notes." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://lilianweng.github.io/posts/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Posts"/>
<meta name="twitter:description" content="Document my learning notes."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://lilianweng.github.io/posts/"
    }
  ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lilianweng.github.io/" accesskey="h" title="Lil&#39;Log (Alt + H)">Lil&#39;Log</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lilianweng.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
            <li>
                <a href="https://www.emojisearch.app/" title="emojisearch.app">
                    <span>emojisearch.app</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>Posts</h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Self-Supervised Representation Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2020-01-09: add a new section on Contrastive Predictive Coding]. [Updated on 2020-04-13: add a “Momentum Contrast” section on MoCo, SimCLR and CURL.] [Updated on 2020-07-08: add a “Bisimulation” section on DeepMDP and DBC.] [Updated on 2020-09-12: add MoCo V2 and BYOL in the “Momentum Contrast” section.] [Updated on 2021-05-31: remove section on “Momentum Contrast” and add a pointer to a full post on “Contrastive Representation Learning”]
Given a task and enough labels, supervised learning can solve it really well....</p>
  </section>
  <footer class="entry-footer">Date: November 10, 2019  |  Estimated Reading Time: 38 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Self-Supervised Representation Learning" href="https://lilianweng.github.io/posts/2019-11-10-self-supervised/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Evolution Strategies
    </h2>
  </header>
  <section class="entry-content">
    <p>Stochastic gradient descent is a universal choice for optimizing deep learning models. However, it is not the only option. With black-box optimization algorithms, you can evaluate a target function $f(x): \mathbb{R}^n \to \mathbb{R}$, even when you don’t know the precise analytic form of $f(x)$ and thus cannot compute gradients or the Hessian matrix. Examples of black-box optimization methods include Simulated Annealing, Hill Climbing and Nelder-Mead method.
Evolution Strategies (ES) is one type of black-box optimization algorithms, born in the family of Evolutionary Algorithms (EA)....</p>
  </section>
  <footer class="entry-footer">Date: September 5, 2019  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Evolution Strategies" href="https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Meta Reinforcement Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>In my earlier post on meta-learning, the problem is mainly defined in the context of few-shot classification. Here I would like to explore more into cases when we try to “meta-learn” Reinforcement Learning (RL) tasks by developing an agent that can solve unseen tasks fast and efficiently.
To recap, a good meta-learning model is expected to generalize to new tasks or new environments that have never been encountered during training....</p>
  </section>
  <footer class="entry-footer">Date: June 23, 2019  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Meta Reinforcement Learning" href="https://lilianweng.github.io/posts/2019-06-23-meta-rl/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Domain Randomization for Sim2Real Transfer
    </h2>
  </header>
  <section class="entry-content">
    <p>In Robotics, one of the hardest problems is how to make your model transfer to the real world. Due to the sample inefficiency of deep RL algorithms and the cost of data collection on real robots, we often need to train models in a simulator which theoretically provides an infinite amount of data. However, the reality gap between the simulator and the physical world often leads to failure when working with physical robots....</p>
  </section>
  <footer class="entry-footer">Date: May 5, 2019  |  Estimated Reading Time: 15 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Domain Randomization for Sim2Real Transfer" href="https://lilianweng.github.io/posts/2019-05-05-domain-randomization/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Are Deep Neural Networks Dramatically Overfitted?
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2019-05-27: add the section on Lottery Ticket Hypothesis.]
If you are like me, entering into the field of deep learning with experience in traditional machine learning, you may often ponder over this question: Since a typical deep neural network has so many parameters and training error can easily be perfect, it should surely suffer from substantial overfitting. How could it be ever generalized to out-of-sample data points?...</p>
  </section>
  <footer class="entry-footer">Date: March 14, 2019  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Are Deep Neural Networks Dramatically Overfitted?" href="https://lilianweng.github.io/posts/2019-03-14-overfit/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Generalized Language Models
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2019-02-14: add ULMFiT and GPT-2.] [Updated on 2020-02-29: add ALBERT.] [Updated on 2020-10-25: add RoBERTa.] [Updated on 2020-12-13: add T5.] [Updated on 2020-12-30: add GPT-3.] [Updated on 2021-11-13: add XLNet, BART and ELECTRA; Also updated the Summary section.]
Fig. 0. I guess they are Elmo &amp; Bert? (Image source: here) We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like OpenAI GPT and BERT have achieved great performance on a variety of language tasks using generic model architectures....</p>
  </section>
  <footer class="entry-footer">Date: January 31, 2019  |  Estimated Reading Time: 36 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Generalized Language Models" href="https://lilianweng.github.io/posts/2019-01-31-lm/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Object Detection Part 4: Fast Detection Models
    </h2>
  </header>
  <section class="entry-content">
    <p>In Part 3, we have reviewed models in the R-CNN family. All of them are region-based object detection algorithms. They can achieve high accuracy but could be too slow for certain applications such as autonomous driving. In Part 4, we only focus on fast object detection models, including SSD, RetinaNet, and models in the YOLO family.
Links to all the posts in the series: [Part 1] [Part 2] [Part 3] [Part 4]....</p>
  </section>
  <footer class="entry-footer">Date: December 27, 2018  |  Estimated Reading Time: 19 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Object Detection Part 4: Fast Detection Models" href="https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Meta-Learning: Learning to Learn Fast
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2019-10-01: thanks to Tianhao, we have this post translated in Chinese!]
A good machine learning model often requires training with a large number of samples. Humans, in contrast, learn new concepts and skills much faster and more efficiently. Kids who have seen cats and birds only a few times can quickly tell them apart. People who know how to ride a bike are likely to discover the way to ride a motorcycle fast with little or even no demonstration....</p>
  </section>
  <footer class="entry-footer">Date: November 30, 2018  |  Estimated Reading Time: 30 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Meta-Learning: Learning to Learn Fast" href="https://lilianweng.github.io/posts/2018-11-30-meta-learning/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Flow-based Deep Generative Models
    </h2>
  </header>
  <section class="entry-content">
    <p>So far, I’ve written about two types of generative models, GAN and VAE. Neither of them explicitly learns the probability density function of real data, $p(\mathbf{x})$ (where $\mathbf{x} \in \mathcal{D}$) — because it is really hard! Taking the generative model with latent variables as an example, $p(\mathbf{x}) = \int p(\mathbf{x}\vert\mathbf{z})p(\mathbf{z})d\mathbf{z}$ can hardly be calculated as it is intractable to go through all possible values of the latent code $\mathbf{z}$....</p>
  </section>
  <footer class="entry-footer">Date: October 13, 2018  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Flow-based Deep Generative Models" href="https://lilianweng.github.io/posts/2018-10-13-flow-models/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>From Autoencoder to Beta-VAE
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2019-07-18: add a section on VQ-VAE &amp; VQ-VAE-2.] [Updated on 2019-07-26: add a section on TD-VAE.] Autocoder is invented to reconstruct high-dimensional data using a neural network model with a narrow bottleneck layer in the middle (oops, this is probably not true for Variational Autoencoder, and we will investigate it in details in later sections). A nice byproduct is dimension reduction: the bottleneck layer captures a compressed latent encoding....</p>
  </section>
  <footer class="entry-footer">Date: August 12, 2018  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to From Autoencoder to Beta-VAE" href="https://lilianweng.github.io/posts/2018-08-12-vae/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Attention? Attention!
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2018-10-28: Add Pointer Network and the link to my implementation of Transformer.] [Updated on 2018-11-06: Add a link to the implementation of Transformer model.] [Updated on 2018-11-18: Add Neural Turing Machines.] [Updated on 2019-07-18: Correct the mistake on using the term “self-attention” when introducing the show-attention-tell paper; moved it to Self-Attention section.] [Updated on 2020-04-07: A follow-up post on improved Transformer models is here.]
Attention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence....</p>
  </section>
  <footer class="entry-footer">Date: June 24, 2018  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Attention? Attention!" href="https://lilianweng.github.io/posts/2018-06-24-attention/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym
    </h2>
  </header>
  <section class="entry-content">
    <p>The full implementation is available in lilianweng/deep-reinforcement-learning-gym
In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].
Environment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e &#34;$(curl -fsSL https://raw....</p>
  </section>
  <footer class="entry-footer">Date: May 5, 2018  |  Estimated Reading Time: 13 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym" href="https://lilianweng.github.io/posts/2018-05-05-drl-implementation/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Policy Gradient Algorithms
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2018-06-30: add two new policy gradient methods, SAC and D4PG.] [Updated on 2018-09-30: add a new policy gradient method, TD3.] [Updated on 2019-02-09: add SAC with automatically adjusted temperature]. [Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in Korean]. [Updated on 2019-09-12: add a new policy gradient method SVPG.] [Updated on 2019-12-22: add a new policy gradient method IMPALA.] [Updated on 2020-10-15: add a new policy gradient method PPG &amp; some new discussion in PPO....</p>
  </section>
  <footer class="entry-footer">Date: April 8, 2018  |  Estimated Reading Time: 52 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Policy Gradient Algorithms" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>A (Long) Peek into Reinforcement Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced. [Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese].
A couple of exciting news in Artificial Intelligence (AI) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge....</p>
  </section>
  <footer class="entry-footer">Date: February 19, 2018  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to A (Long) Peek into Reinforcement Learning" href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>The Multi-Armed Bandit Problem and Its Solutions
    </h2>
  </header>
  <section class="entry-content">
    <p>The algorithms are implemented for Bernoulli bandit in lilianweng/multi-armed-bandit.
Exploitation vs Exploration The exploration vs exploitation dilemma exists in many aspects of our life. Say, your favorite restaurant is right around the corner. If you go there every day, you would be confident of what you will get, but miss the chances of discovering an even better option. If you try new places all the time, very likely you are gonna have to eat unpleasant food from time to time....</p>
  </section>
  <footer class="entry-footer">Date: January 23, 2018  |  Estimated Reading Time: 10 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to The Multi-Armed Bandit Problem and Its Solutions" href="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Object Detection for Dummies Part 3: R-CNN Family
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2018-12-20: Remove YOLO here. Part 4 will cover multiple fast object detection algorithms, including YOLO.] [Updated on 2018-12-27: Add bbox regression and tricks sections for R-CNN.]
In the series of “Object Detection for Dummies”, we started with basic concepts in image processing, such as gradient vectors and HOG, in Part 1. Then we introduced classic convolutional neural network architecture designs for classification and pioneer models for object recognition, Overfeat and DPM, in Part 2....</p>
  </section>
  <footer class="entry-footer">Date: December 31, 2017  |  Estimated Reading Time: 13 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Object Detection for Dummies Part 3: R-CNN Family" href="https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Object Detection for Dummies Part 2: CNN, DPM and Overfeat
    </h2>
  </header>
  <section class="entry-content">
    <p>Part 1 of the “Object Detection for Dummies” series introduced: (1) the concept of image gradient vector and how HOG algorithm summarizes the information across all the gradient vectors in one image; (2) how the image segmentation algorithm works to detect regions that potentially contain objects; (3) how the Selective Search algorithm refines the outcomes of image segmentation for better region proposal.
In Part 2, we are about to find out more on the classic convolution neural network architectures for image classification....</p>
  </section>
  <footer class="entry-footer">Date: December 15, 2017  |  Estimated Reading Time: 7 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Object Detection for Dummies Part 2: CNN, DPM and Overfeat" href="https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS
    </h2>
  </header>
  <section class="entry-content">
    <p>I’ve never worked in the field of computer vision and has no idea how the magic could work when an autonomous car is configured to tell apart a stop sign from a pedestrian in a red hat. To motivate myself to look into the maths behind object recognition and detection algorithms, I’m writing a few posts on this topic “Object Detection for Dummies”. This post, part 1, starts with super rudimentary concepts in image processing and a few methods for image segmentation....</p>
  </section>
  <footer class="entry-footer">Date: October 29, 2017  |  Estimated Reading Time: 15 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS" href="https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Learning Word Embedding
    </h2>
  </header>
  <section class="entry-content">
    <p>Human vocabulary comes in free text. In order to make a machine learning model understand and process the natural language, we need to transform the free-text words into numeric values. One of the simplest transformation approaches is to do a one-hot encoding in which each distinct word stands for one dimension of the resulting vector and a binary value indicates whether the word presents (1) or not (0).
However, one-hot encoding is impractical computationally when dealing with the entire vocabulary, as the representation demands hundreds of thousands of dimensions....</p>
  </section>
  <footer class="entry-footer">Date: October 15, 2017  |  Estimated Reading Time: 18 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Learning Word Embedding" href="https://lilianweng.github.io/posts/2017-10-15-word-embedding/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Anatomize Deep Learning with Information Theory
    </h2>
  </header>
  <section class="entry-content">
    <p>Professor Naftali Tishby passed away in 2021. Hope the post can introduce his cool idea of information bottleneck to more people.
Recently I watched the talk “Information Theory in Deep Learning” by Prof Naftali Tishby and found it very interesting. He presented how to apply the information theory to study the growth and transformation of deep neural networks during training. Using the Information Bottleneck (IB) method, he proposed a new learning bound for deep neural networks (DNN), as the traditional learning theory fails due to the exponentially large number of parameters....</p>
  </section>
  <footer class="entry-footer">Date: September 28, 2017  |  Estimated Reading Time: 9 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Anatomize Deep Learning with Information Theory" href="https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="https://lilianweng.github.io/posts/">« </a>
    <a class="next" href="https://lilianweng.github.io/posts/page/3/"> »</a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://lilianweng.github.io/">Lil&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
