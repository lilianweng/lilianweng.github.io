<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>reinforcement-learning | Lil&#39;Log</title>
<meta name="keywords" content="" />
<meta name="description" content="Document my learning notes.">
<meta name="author" content="Lilian Weng">
<link rel="canonical" href="https://lilianweng.github.io/tags/reinforcement-learning/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css" integrity="sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://lilianweng.github.io/favicon_peach.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://lilianweng.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://lilianweng.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://lilianweng.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://lilianweng.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://lilianweng.github.io/tags/reinforcement-learning/index.xml">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-HFT45VFBX6', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="reinforcement-learning" />
<meta property="og:description" content="Document my learning notes." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://lilianweng.github.io/tags/reinforcement-learning/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="reinforcement-learning"/>
<meta name="twitter:description" content="Document my learning notes."/>

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://lilianweng.github.io/" accesskey="h" title="Lil&#39;Log (Alt + H)">Lil&#39;Log</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://lilianweng.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://lilianweng.github.io/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
            <li>
                <a href="https://www.emojisearch.app/" title="emojisearch.app">
                    <span>emojisearch.app</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>reinforcement-learning</h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>The Transformer Family Version 2.0
    </h2>
  </header>
  <section class="entry-content">
    <p>Many new Transformer architecture improvements have been proposed since my last post on “The Transformer Family” about three years ago. Here I did a big refactoring and enrichment of that 2020 post — restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.
Notations Symbol Meaning $d$ The model size / hidden state dimension / positional encoding size....</p>
  </section>
  <footer class="entry-footer">Date: January 27, 2023  |  Estimated Reading Time: 45 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to The Transformer Family Version 2.0" href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Controllable Neural Text Generation
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.] [Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the “prompt design” section.] [Updated on 2021-09-19: Add “unlikelihood training”.]
There is a gigantic amount of free text on the Web, several magnitude more than labelled benchmark datasets. The state-of-the-art language models (LM) are trained with unsupervised Web data in large scale. When generating samples from LM by iteratively sampling the next token, we do not have much control over attributes of the output text, such as the topic, the style, the sentiment, etc....</p>
  </section>
  <footer class="entry-footer">Date: January 2, 2021  |  Estimated Reading Time: 42 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Controllable Neural Text Generation" href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Neural Architecture Search
    </h2>
  </header>
  <section class="entry-content">
    <p>Although most popular and successful model architectures are designed by human experts, it doesn’t mean we have explored the entire network architecture space and settled down with the best option. We would have a better chance to find the optimal solution if we adopt a systematic and automatic way of learning high-performance model architectures.
Automatically learning and evolving network topologies is not a new idea (Stanley &amp; Miikkulainen, 2002). In recent years, the pioneering work by Zoph &amp; Le 2017 and Baker et al....</p>
  </section>
  <footer class="entry-footer">Date: August 6, 2020  |  Estimated Reading Time: 32 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Neural Architecture Search" href="https://lilianweng.github.io/posts/2020-08-06-nas/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Exploration Strategies in Deep Reinforcement Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2020-06-17: Add “exploration via disagreement” in the “Forward Dynamics” section.
Exploitation versus exploration is a critical topic in Reinforcement Learning. We’d like the RL agent to find the best solution as fast as possible. However, in the meantime, committing to solutions too quickly without enough exploration sounds pretty bad, as it could lead to local minima or total failure. Modern RL algorithms that optimize for the best returns can achieve good exploitation quite efficiently, while exploration remains more like an open topic....</p>
  </section>
  <footer class="entry-footer">Date: June 7, 2020  |  Estimated Reading Time: 36 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Exploration Strategies in Deep Reinforcement Learning" href="https://lilianweng.github.io/posts/2020-06-07-exploration-drl/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>The Transformer Family
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2023-01-27: After almost three years, I did a big refactoring update of this post to incorporate a bunch of new Transformer models since 2020. The enhanced version of this post is here: The Transformer Family Version 2.0. Please refer to that post on this topic.] It has been almost two years since my last post on attention. Recent progress on new and enhanced versions of Transformer motivates me to write another post on this specific topic, focusing on how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving and more....</p>
  </section>
  <footer class="entry-footer">Date: April 7, 2020  |  Estimated Reading Time: 25 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to The Transformer Family" href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Curriculum for Reinforcement Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2020-02-03: mentioning PCG in the “Task-Specific Curriculum” section. [Updated on 2020-02-04: Add a new “curriculum through distillation” section.
It sounds like an impossible task if we want to teach integral or derivative to a 3-year-old who does not even know basic arithmetics. That’s why education is important, as it provides a systematic way to break down complex knowledge and a nice curriculum for teaching concepts from simple to hard....</p>
  </section>
  <footer class="entry-footer">Date: January 29, 2020  |  Estimated Reading Time: 24 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Curriculum for Reinforcement Learning" href="https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Self-Supervised Representation Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2020-01-09: add a new section on Contrastive Predictive Coding]. [Updated on 2020-04-13: add a “Momentum Contrast” section on MoCo, SimCLR and CURL.] [Updated on 2020-07-08: add a “Bisimulation” section on DeepMDP and DBC.] [Updated on 2020-09-12: add MoCo V2 and BYOL in the “Momentum Contrast” section.] [Updated on 2021-05-31: remove section on “Momentum Contrast” and add a pointer to a full post on “Contrastive Representation Learning”]
Given a task and enough labels, supervised learning can solve it really well....</p>
  </section>
  <footer class="entry-footer">Date: November 10, 2019  |  Estimated Reading Time: 38 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Self-Supervised Representation Learning" href="https://lilianweng.github.io/posts/2019-11-10-self-supervised/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Evolution Strategies
    </h2>
  </header>
  <section class="entry-content">
    <p>Stochastic gradient descent is a universal choice for optimizing deep learning models. However, it is not the only option. With black-box optimization algorithms, you can evaluate a target function $f(x): \mathbb{R}^n \to \mathbb{R}$, even when you don’t know the precise analytic form of $f(x)$ and thus cannot compute gradients or the Hessian matrix. Examples of black-box optimization methods include Simulated Annealing, Hill Climbing and Nelder-Mead method.
Evolution Strategies (ES) is one type of black-box optimization algorithms, born in the family of Evolutionary Algorithms (EA)....</p>
  </section>
  <footer class="entry-footer">Date: September 5, 2019  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Evolution Strategies" href="https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Meta Reinforcement Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>In my earlier post on meta-learning, the problem is mainly defined in the context of few-shot classification. Here I would like to explore more into cases when we try to “meta-learn” Reinforcement Learning (RL) tasks by developing an agent that can solve unseen tasks fast and efficiently.
To recap, a good meta-learning model is expected to generalize to new tasks or new environments that have never been encountered during training....</p>
  </section>
  <footer class="entry-footer">Date: June 23, 2019  |  Estimated Reading Time: 22 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Meta Reinforcement Learning" href="https://lilianweng.github.io/posts/2019-06-23-meta-rl/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Domain Randomization for Sim2Real Transfer
    </h2>
  </header>
  <section class="entry-content">
    <p>In Robotics, one of the hardest problems is how to make your model transfer to the real world. Due to the sample inefficiency of deep RL algorithms and the cost of data collection on real robots, we often need to train models in a simulator which theoretically provides an infinite amount of data. However, the reality gap between the simulator and the physical world often leads to failure when working with physical robots....</p>
  </section>
  <footer class="entry-footer">Date: May 5, 2019  |  Estimated Reading Time: 15 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Domain Randomization for Sim2Real Transfer" href="https://lilianweng.github.io/posts/2019-05-05-domain-randomization/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym
    </h2>
  </header>
  <section class="entry-content">
    <p>The full implementation is available in lilianweng/deep-reinforcement-learning-gym
In the previous two posts, I have introduced the algorithms of many deep reinforcement learning models. Now it is the time to get our hands dirty and practice how to implement the models in the wild. The implementation is gonna be built in Tensorflow and OpenAI gym environment. The full version of the code in this tutorial is available in [lilian/deep-reinforcement-learning-gym].
Environment Setup Make sure you have Homebrew installed: /usr/bin/ruby -e &#34;$(curl -fsSL https://raw....</p>
  </section>
  <footer class="entry-footer">Date: May 5, 2018  |  Estimated Reading Time: 13 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Implementing Deep Reinforcement Learning Models with Tensorflow &#43; OpenAI Gym" href="https://lilianweng.github.io/posts/2018-05-05-drl-implementation/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>Policy Gradient Algorithms
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2018-06-30: add two new policy gradient methods, SAC and D4PG.] [Updated on 2018-09-30: add a new policy gradient method, TD3.] [Updated on 2019-02-09: add SAC with automatically adjusted temperature]. [Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in Korean]. [Updated on 2019-09-12: add a new policy gradient method SVPG.] [Updated on 2019-12-22: add a new policy gradient method IMPALA.] [Updated on 2020-10-15: add a new policy gradient method PPG &amp; some new discussion in PPO....</p>
  </section>
  <footer class="entry-footer">Date: April 8, 2018  |  Estimated Reading Time: 52 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to Policy Gradient Algorithms" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>A (Long) Peek into Reinforcement Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced. [Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese].
A couple of exciting news in Artificial Intelligence (AI) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge....</p>
  </section>
  <footer class="entry-footer">Date: February 19, 2018  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to A (Long) Peek into Reinforcement Learning" href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2>The Multi-Armed Bandit Problem and Its Solutions
    </h2>
  </header>
  <section class="entry-content">
    <p>The algorithms are implemented for Bernoulli bandit in lilianweng/multi-armed-bandit.
Exploitation vs Exploration The exploration vs exploitation dilemma exists in many aspects of our life. Say, your favorite restaurant is right around the corner. If you go there every day, you would be confident of what you will get, but miss the chances of discovering an even better option. If you try new places all the time, very likely you are gonna have to eat unpleasant food from time to time....</p>
  </section>
  <footer class="entry-footer">Date: January 23, 2018  |  Estimated Reading Time: 10 min  |  Author: Lilian Weng</footer>
  <a class="entry-link" aria-label="post link to The Multi-Armed Bandit Problem and Its Solutions" href="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/"></a>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://lilianweng.github.io/">Lil&#39;Log</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
