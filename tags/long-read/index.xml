<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>long-read on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/long-read/</link>
    <description>Recent content in long-read on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/long-read/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Transformer Family Version 2.0</title>
      <link>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</link>
      <pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</guid>
      <description>Many new Transformer architecture improvements have been proposed since my last post on &amp;ldquo;The Transformer Family&amp;rdquo; about three years ago. Here I did a big refactoring and enrichment of that 2020 post &amp;mdash; restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.
Notations Symbol Meaning $d$ The model size / hidden state dimension / positional encoding size.</description>
    </item>
    
    <item>
      <title>Large Transformer Model Inference Optimization</title>
      <link>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</link>
      <pubDate>Tue, 10 Jan 2023 10:00:00 -0700</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</guid>
      <description>[Updated on 2023-01-24: add a small section on Distillation.]
Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use. The extremely high inference cost, in both time and memory, is a big bottleneck for adopting a powerful transformer for solving real-world tasks at scale.
Why is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge (Pope et al.</description>
    </item>
    
    <item>
      <title>Contrastive Representation Learning</title>
      <link>https://lilianweng.github.io/posts/2021-05-31-contrastive/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-05-31-contrastive/</guid>
      <description>The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning.
Contrastive Training Objectives In early versions of loss functions for contrastive learning, only one positive and one negative sample are involved.</description>
    </item>
    
    <item>
      <title>Controllable Neural Text Generation</title>
      <link>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/</guid>
      <description>[Updated on 2021-02-01: Updated to version 2.0 with several work added and many typos fixed.] [Updated on 2021-05-26: Add P-tuning and Prompt Tuning in the &amp;ldquo;prompt design&amp;rdquo; section.] [Updated on 2021-09-19: Add &amp;ldquo;unlikelihood training&amp;rdquo;.]
There is a gigantic amount of free text on the Web, several magnitude more than labelled benchmark datasets. The state-of-the-art language models (LM) are trained with unsupervised Web data in large scale. When generating samples from LM by iteratively sampling the next token, we do not have much control over attributes of the output text, such as the topic, the style, the sentiment, etc.</description>
    </item>
    
    <item>
      <title>Exploration Strategies in Deep Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-06-07-exploration-drl/</guid>
      <description>[Updated on 2020-06-17: Add &amp;ldquo;exploration via disagreement&amp;rdquo; in the &amp;ldquo;Forward Dynamics&amp;rdquo; section.
Exploitation versus exploration is a critical topic in Reinforcement Learning. We&amp;rsquo;d like the RL agent to find the best solution as fast as possible. However, in the meantime, committing to solutions too quickly without enough exploration sounds pretty bad, as it could lead to local minima or total failure. Modern RL algorithms that optimize for the best returns can achieve good exploitation quite efficiently, while exploration remains more like an open topic.</description>
    </item>
    
    <item>
      <title>Self-Supervised Representation Learning</title>
      <link>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</link>
      <pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-11-10-self-supervised/</guid>
      <description>[Updated on 2020-01-09: add a new section on Contrastive Predictive Coding]. [Updated on 2020-04-13: add a &amp;ldquo;Momentum Contrast&amp;rdquo; section on MoCo, SimCLR and CURL.] [Updated on 2020-07-08: add a &amp;ldquo;Bisimulation&amp;rdquo; section on DeepMDP and DBC.] [Updated on 2020-09-12: add MoCo V2 and BYOL in the &amp;ldquo;Momentum Contrast&amp;rdquo; section.] [Updated on 2021-05-31: remove section on &amp;ldquo;Momentum Contrast&amp;rdquo; and add a pointer to a full post on &amp;ldquo;Contrastive Representation Learning&amp;rdquo;]
Given a task and enough labels, supervised learning can solve it really well.</description>
    </item>
    
    <item>
      <title>Generalized Language Models</title>
      <link>https://lilianweng.github.io/posts/2019-01-31-lm/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-01-31-lm/</guid>
      <description>[Updated on 2019-02-14: add ULMFiT and GPT-2.] [Updated on 2020-02-29: add ALBERT.] [Updated on 2020-10-25: add RoBERTa.] [Updated on 2020-12-13: add T5.] [Updated on 2020-12-30: add GPT-3.] [Updated on 2021-11-13: add XLNet, BART and ELECTRA; Also updated the Summary section.]
Fig. 0. I guess they are Elmo &amp; Bert? (Image source: here) We have seen amazing progress in NLP in 2018. Large-scale pre-trained language modes like OpenAI GPT and BERT have achieved great performance on a variety of language tasks using generic model architectures.</description>
    </item>
    
    <item>
      <title>Meta-Learning: Learning to Learn Fast</title>
      <link>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</link>
      <pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-11-30-meta-learning/</guid>
      <description>[Updated on 2019-10-01: thanks to Tianhao, we have this post translated in Chinese!]
A good machine learning model often requires training with a large number of samples. Humans, in contrast, learn new concepts and skills much faster and more efficiently. Kids who have seen cats and birds only a few times can quickly tell them apart. People who know how to ride a bike are likely to discover the way to ride a motorcycle fast with little or even no demonstration.</description>
    </item>
    
    <item>
      <title>Policy Gradient Algorithms</title>
      <link>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</guid>
      <description>[Updated on 2018-06-30: add two new policy gradient methods, SAC and D4PG.] [Updated on 2018-09-30: add a new policy gradient method, TD3.] [Updated on 2019-02-09: add SAC with automatically adjusted temperature]. [Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in Korean]. [Updated on 2019-09-12: add a new policy gradient method SVPG.] [Updated on 2019-12-22: add a new policy gradient method IMPALA.] [Updated on 2020-10-15: add a new policy gradient method PPG &amp;amp; some new discussion in PPO.</description>
    </item>
    
    <item>
      <title>A (Long) Peek into Reinforcement Learning</title>
      <link>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2018-02-19-rl-overview/</guid>
      <description>[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced. [Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese].
A couple of exciting news in Artificial Intelligence (AI) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge.</description>
    </item>
    
    <item>
      <title>From GAN to WGAN</title>
      <link>https://lilianweng.github.io/posts/2017-08-20-gan/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-08-20-gan/</guid>
      <description>[Updated on 2018-09-30: thanks to Yoonju, we have this post translated in Korean!] [Updated on 2019-04-18: this post is also available on arXiv.]
Generative adversarial network (GAN) has shown great results in many generative tasks to replicate the real-world rich content such as images, human language, and music. It is inspired by game theory: two models, a generator and a critic, are competing with each other while making each other stronger at the same time.</description>
    </item>
    
  </channel>
</rss>
