<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Foundation on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/foundation/</link>
    <description>Recent content in Foundation on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/foundation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Transformer Family Version 2.0</title>
      <link>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</link>
      <pubDate>Fri, 27 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</guid>
      <description>&lt;p&gt;Many new Transformer architecture improvements have been proposed since my last post on &lt;a href=&#34;https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/&#34;&gt;&lt;ins&gt;&amp;ldquo;The Transformer Family&amp;rdquo;&lt;/ins&gt;&lt;/a&gt; about three years ago. Here I did a big refactoring and enrichment of that 2020 post &amp;mdash; restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.&lt;/p&gt;
&lt;h1 id=&#34;notations&#34;&gt;Notations&lt;/h1&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Symbol&lt;/th&gt;
          &lt;th&gt;Meaning&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;$d$&lt;/td&gt;
          &lt;td&gt;The model size / hidden state dimension / positional encoding size.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$h$&lt;/td&gt;
          &lt;td&gt;The number of heads in multi-head attention layer.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$L$&lt;/td&gt;
          &lt;td&gt;The segment length of input sequence.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$N$&lt;/td&gt;
          &lt;td&gt;The total number of attention layers in the model; not considering MoE.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{X} \in \mathbb{R}^{L \times d}$&lt;/td&gt;
          &lt;td&gt;The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$&lt;/td&gt;
          &lt;td&gt;The key weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$&lt;/td&gt;
          &lt;td&gt;The query weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$&lt;/td&gt;
          &lt;td&gt;The value weight matrix. Often we have $d_k = d_v = d$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$&lt;/td&gt;
          &lt;td&gt;The weight matrices per head.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$&lt;/td&gt;
          &lt;td&gt;The output weight matrix.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$&lt;/td&gt;
          &lt;td&gt;The query embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$&lt;/td&gt;
          &lt;td&gt;The key embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$&lt;/td&gt;
          &lt;td&gt;The value embedding inputs.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$&lt;/td&gt;
          &lt;td&gt;Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$S_i$&lt;/td&gt;
          &lt;td&gt;A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{A} \in \mathbb{R}^{L \times L}$&lt;/td&gt;
          &lt;td&gt;The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$a_{ij} \in \mathbf{A}$&lt;/td&gt;
          &lt;td&gt;The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;$\mathbf{P} \in \mathbb{R}^{L \times d}$&lt;/td&gt;
          &lt;td&gt;position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;transformer-basics&#34;&gt;Transformer Basics&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;Transformer&lt;/strong&gt; (which will be referred to as &amp;ldquo;vanilla Transformer&amp;rdquo; to distinguish it from other enhanced versions; &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Vaswani, et al., 2017&lt;/a&gt;) model has an encoder-decoder architecture, as commonly used in many &lt;a href=&#34;https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation&#34;&gt;NMT&lt;/a&gt; models. Later simplified Transformer was shown to achieve great performance in language modeling tasks, like in encoder-only &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/#bert&#34;&gt;BERT&lt;/a&gt; or decoder-only &lt;a href=&#34;https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt&#34;&gt;GPT&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Large Transformer Model Inference Optimization</title>
      <link>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</link>
      <pubDate>Tue, 10 Jan 2023 10:00:00 -0700</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</guid>
      <description>&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2023-01-24: add a small section on &lt;a href=&#34;#distillation&#34;&gt;Distillation&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Large transformer models are mainstream nowadays, creating SoTA results for a variety of tasks. They are powerful but very expensive to train and use. The extremely high inference cost, in both time and memory, is a big bottleneck for adopting a powerful transformer for solving real-world tasks at scale.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is it hard to run inference for large transformer models?&lt;/strong&gt; Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge (&lt;a href=&#34;https://arxiv.org/abs/2211.05102&#34;&gt;Pope et al. 2022&lt;/a&gt;):&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Some Math behind Neural Tangent Kernel</title>
      <link>https://lilianweng.github.io/posts/2022-09-08-ntk/</link>
      <pubDate>Thu, 08 Sep 2022 10:00:00 -0700</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2022-09-08-ntk/</guid>
      <description>&lt;p&gt;Neural networks are &lt;a href=&#34;https://lilianweng.github.io/posts/2019-03-14-overfit/&#34;&gt;well known&lt;/a&gt; to be over-parameterized and can often easily fit data with near-zero training loss with decent generalization performance on test dataset. Although all these parameters are initialized at random, the optimization process can consistently lead to similarly good outcomes. And this is true even when the number of model parameters exceeds the number of training data points.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Neural tangent kernel (NTK)&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/1806.07572&#34;&gt;Jacot et al. 2018&lt;/a&gt;) is a kernel to explain the evolution of neural networks during training via gradient descent. It leads to great insights into why neural networks with enough width can consistently converge to a global minimum when trained to minimize an empirical loss. In the post, we will do a deep dive into the motivation and definition of NTK, as well as the proof of a deterministic convergence at different initializations of neural networks with infinite width by characterizing NTK in such a setting.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to Train Really Large Models on Many GPUs?</title>
      <link>https://lilianweng.github.io/posts/2021-09-25-train-large/</link>
      <pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2021-09-25-train-large/</guid>
      <description>&lt;!-- How to train large and deep neural networks is challenging, as it demands a large amount of GPU memory and a long horizon of training time. This post reviews several popular training parallelism paradigms, as well as a variety of model architecture and memory saving designs to make it possible to train very large neural networks across a large number of GPUs. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2022-03-13: add &lt;a href=&#34;#ec&#34;&gt;expert choice routing&lt;/a&gt;.]&lt;/span&gt;&lt;br/&gt;
&lt;span class=&#34;update&#34;&gt;[Updated on 2022-06-10]: &lt;a href=&#34;https://gregbrockman.com/&#34;&gt;Greg&lt;/a&gt; and I wrote a shorted and upgraded version of this post, published on OpenAI Blog: &lt;a href=&#34;https://openai.com/blog/techniques-for-training-large-neural-networks/&#34;&gt;&amp;ldquo;Techniques for Training Large Neural Networks&amp;rdquo;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Transformer Family</title>
      <link>https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/</guid>
      <description>&lt;!-- Inspired by recent progress on various enhanced versions of Transformer models, this post presents how the vanilla Transformer can be improved for longer-term attention span, less memory and computation consumption, RL task solving, etc. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on &lt;mark&gt;&lt;strong&gt;2023-01-27&lt;/strong&gt;&lt;/mark&gt;: After almost three years, I did a big refactoring update of this post to incorporate a bunch of new Transformer models since 2020. The enhanced version of this post is here: &lt;a href=&#34;https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/&#34;&gt;&lt;mark&gt;&lt;b&gt;The Transformer Family Version 2.0&lt;/b&gt;&lt;/mark&gt;&lt;/a&gt;. Please refer to that post on this topic.]&lt;/span&gt;
&lt;br/&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Are Deep Neural Networks Dramatically Overfitted?</title>
      <link>https://lilianweng.github.io/posts/2019-03-14-overfit/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2019-03-14-overfit/</guid>
      <description>&lt;!-- If you are, like me, confused by why deep neural networks can generalize to out-of-sample data points without drastic overfitting, keep on reading. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;[Updated on 2019-05-27: add the &lt;a href=&#34;#the-lottery-ticket-hypothesis&#34;&gt;section&lt;/a&gt; on Lottery Ticket Hypothesis.]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you are like me, entering into the field of deep learning with experience in traditional machine learning, you may often ponder over this question: Since a typical deep neural network has so many parameters and training error can easily be perfect, it should surely suffer from substantial overfitting. How could it be ever generalized to out-of-sample data points?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Anatomize Deep Learning with Information Theory</title>
      <link>https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/</guid>
      <description>&lt;!-- This post is a summary of Prof Naftali Tishby&#39;s recent talk on &#34;Information Theory in Deep Learning&#34;. It presented how to apply the information theory to study the growth and transformation of deep neural networks during training. --&gt;
&lt;p&gt;&lt;span class=&#34;update&#34;&gt;Professor Naftali Tishby passed away in 2021. Hope the post can introduce his cool idea of information bottleneck to more people.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recently I watched the talk &lt;a href=&#34;https://youtu.be/bLqJHjXihK8&#34;&gt;&amp;ldquo;Information Theory in Deep Learning&amp;rdquo;&lt;/a&gt; by Prof Naftali Tishby and found it very interesting. He presented how to apply the information theory to study the growth and transformation of deep neural networks during training. Using the &lt;a href=&#34;https://arxiv.org/pdf/physics/0004057.pdf&#34;&gt;Information Bottleneck (IB)&lt;/a&gt; method, he proposed a new learning bound for deep neural networks (DNN), as the traditional learning theory fails due to the exponentially large number of parameters. Another keen observation is that DNN training involves two distinct phases: First, the network is trained to fully represent the input data and minimize the generalization error; then, it learns to forget the irrelevant details by compressing the representation of the input.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to Explain the Prediction of a Machine Learning Model?</title>
      <link>https://lilianweng.github.io/posts/2017-08-01-interpretation/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-08-01-interpretation/</guid>
      <description>&lt;!-- This post reviews some research in model interpretability, covering two aspects: (i) interpretable models with model-specific interpretation methods and (ii) approaches of explaining black-box models. I included an open discussion on explainable artificial intelligence at the end. --&gt;
&lt;p&gt;The machine learning models have started penetrating into critical areas like health care, justice systems, and financial industry. Thus to figure out how the models make the decisions and make sure the decisioning process is aligned with the ethnic requirements or legal regulations becomes a necessity.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>An Overview of Deep Learning for Curious People</title>
      <link>https://lilianweng.github.io/posts/2017-06-21-overview/</link>
      <pubDate>Wed, 21 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2017-06-21-overview/</guid>
      <description>&lt;!-- Starting earlier this year, I grew a strong curiosity of deep learning and spent some time reading about this field. To document what Iâ€™ve learned and to provide some interesting pointers to people with similar interests, I wrote this overview of deep learning models and their applications. --&gt;
&lt;p&gt;&lt;span style=&#34;color: #aaaaaa;&#34;&gt;(The post was originated from my talk for &lt;a href=&#34;http://wimlds.org/chapters/about-bay-area/&#34;&gt;WiMLDS x Fintech meetup&lt;/a&gt; hosted by &lt;a href=&#34;www.affirm.com&#34;&gt;Affirm&lt;/a&gt;.)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I believe many of you have watched or heard of the &lt;a href=&#34;https://youtu.be/vFr3K2DORc8&#34;&gt;games&lt;/a&gt; between AlphaGo and professional Go player &lt;a href=&#34;https://en.wikipedia.org/wiki/Lee_Sedol&#34;&gt;Lee Sedol&lt;/a&gt; in 2016. Lee has the highest rank of nine dan and many world championships. No doubt, he is one of the best Go players in the world, but he &lt;a href=&#34;https://www.scientificamerican.com/article/how-the-computer-beat-the-go-master/&#34;&gt;lost by 1-4&lt;/a&gt; in this series versus AlphaGo. Before this, Go was considered to be an intractable game for computers to master, as its simple rules lay out an exponential number of variations in the board positions, many more than what in Chess. This event surely highlighted 2016 as a big year for AI. Because of AlphaGo, much attention has been attracted to the progress of AI.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
