<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>neural-tangent-kernel on Lil&#39;Log</title>
    <link>https://lilianweng.github.io/tags/neural-tangent-kernel/</link>
    <description>Recent content in neural-tangent-kernel on Lil&#39;Log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Sep 2022 10:00:00 -0700</lastBuildDate><atom:link href="https://lilianweng.github.io/tags/neural-tangent-kernel/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some Math behind Neural Tangent Kernel</title>
      <link>https://lilianweng.github.io/posts/2022-09-08-ntk/</link>
      <pubDate>Thu, 08 Sep 2022 10:00:00 -0700</pubDate>
      
      <guid>https://lilianweng.github.io/posts/2022-09-08-ntk/</guid>
      <description>Neural networks are well known to be over-parameterized and can often easily fit data with near-zero training loss with decent generalization performance on test dataset. Although all these parameters are initialized at random, the optimization process can consistently lead to similarly good outcomes. And this is true even when the number of model parameters exceeds the number of training data points.
Neural tangent kernel (NTK) (Jacot et al. 2018) is a kernel to explain the evolution of neural networks during training via gradient descent.</description>
    </item>
    
  </channel>
</rss>
